<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott&#39;s Blog</title>
    <description>I am a graduate student at UW Madison studying computing applications to physical geography and paleoecological change.
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 19 Sep 2016 20:35:53 -0500</pubDate>
    <lastBuildDate>Mon, 19 Sep 2016 20:35:53 -0500</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>PhD Preview</title>
        <description>&lt;p&gt;I’ve been thinking a lot about my PhD research, even I’ve been working 12 hours a day to finish my master’s thesis. If you were recently thinking &lt;em&gt;“I wonder what Scott will be doing in Wisconsin for the next 3-6 years”&lt;/em&gt;, wonder no more. This is the Cliff Notes version, expect a 150-200 page version in early 2021.&lt;/p&gt;

&lt;p&gt;Contemporary understanding of past land cover dynamics comes primarily from expert interpretation of paleoenvironmental proxies, such as fossil pollen or vertebrate macrofossils. My dissertation research will focus on the link between the paleoecological proxy data, particularly fossil pollen, and the drivers of land use change by developing a hierarchical Bayesian model that calibrates dense networks of modern pollen samples with MODIS remote sensing data.  Once calibrated, the model will be used to develop a gridded dataset that reconstructs tree cover percentage and relative composition of broadleaf and needleleaf plant types at 500-year intervals since the Last Glacial Maximum (22kya). The reconstructions can then be used to to identify and test the drivers of regional and global scale land cover shifts, in response to climate and other forcings. I plan to test the impact of vegetation change during the late Pleistocene to draw inference about megafaunal decline and extinction. My work bridges a key gap by providing a mechanism to propagate our understanding of modern land cover change, in the form of remote sensing data products, back through time while estimating model uncertainty.  Using land cover dynamics from the recent geologic past can help to forecast how land cover and biodiversity will change when exposed to 21st century climatic and anthropogenic forcings.&lt;/p&gt;

&lt;h4 id=&quot;research-plan&quot;&gt;Research Plan&lt;/h4&gt;
&lt;p&gt;I plan to split my research into three phases, each lasting approximately one year.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Model Development and Parameterization: During this phase, I will develop a spatiotemporal Bayesian hierarchical regression model to reconstruct past land cover, focusing on model development, calibration, and parameterization.  I will use the Bayesian framework to assimilate modern land cover datasets, derived from Moderate Resolution Imaging Spectroradiometer (MODIS) satellite data and modern pollen surface samples from the Neotoma Paleoecological Database and the North American Modern Pollen Database. The MODIS data is an gridded product describing the relative proportion of broad leaved, needle leaved, coniferous, and deciduous tree cover, and non-arboreal land cover in each 5’ grid cell. The modern pollen surface samples, which record the relative abundance of taxa in the local biota, will be used to regress the remote sensing data onto the sedimentary record, allowing me to calibrate a predictive model to infer land cover at 500-year intervals from the Last Glacial Maximum to the present in North America. The model will specifically incorporate spatial and temporal uncertainty in pollen source area and dispersal kernels, temporal and dating uncertainty in the fossil record, and uncertainties inherent in the classification process used by MODIS. Specific care will be taken to develop a robust model that can be applied both the relative homogeneous eastern forests and the highly complex and mountainous vegetation mosaics in the western areas of the continent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model Assessment and Human Impacts: I will also look specifically at the model’s reconstructions of areas of known human development to determine the model’s success in capturing human landscape-scale modification processes, such as burning and agriculture.  I will work with the archeological literature and paleoenvironmental syntheses, such as those using fossil charcoal, that document human landscape alteration, to compare know areas of impact with the model’s estimates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Late-Pleistocene Megafauna Extinction: I will demonstrate the usefulness of the model by applying its output to an often-studied problem in paleoecological change: determining the key drivers of megafuana extinction during the late Pleistocene.  Models of megafauna decline are currently often only informed by climate model output and expert-inferred climate and vegetation characteristics from paleoecological proxies, such as fossil pollen. I will use the land cover reconstructions I develop to assess the degree to which land cover change was a driver of fuanal decline.  Using species distribution modeling techniques, I will evaluate the degree to which vegetation and climate contribute to the decline individually and when they are allowed to interact. This phase will demonstrate the model’s application and reconstructions ability to be incorporated as an input into other models, and provide new insight into a lingering question in global change research.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;importance&quot;&gt;Importance&lt;/h4&gt;
&lt;p&gt;In addition to contributing to scientific understanding of human-induced change and faunal declines, the techniques and datasets produced in the course of this research are essential for improving the estimates of carbon sequestration and warming during the 21st century produced by ecosystem and global circulation climate models. The advantages of Bayesian hierarchical models for reconstructing past landscapes have been demonstrated, though they have yet to be applied to the continental scale and to the problem of land cover in North America, though other techniques have been applied to European land cover during the Holocene. Biogeophysical feedback cycles, particularly the carbon cycle, can have considerable effects, though their operational scale is too small to be effectively captured by the still relatively rough resolution of global earth system models, so independent reconstructions are of high priority to climate modelers. Additionally, hypotheses about carbon sequestration and climate change mitigation strategies cannot be validated without further understanding of past land cover dynamics.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Sep 2016 09:22:52 -0500</pubDate>
        <link>/research/2016/09/19/PHD-Preview.html</link>
        <guid isPermaLink="true">/research/2016/09/19/PHD-Preview.html</guid>
        
        
        <category>Research</category>
        
      </item>
    
      <item>
        <title>William&#39;s Lab Github Practical</title>
        <description>&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Introduce Git and Github: what are they, what’s the difference, and how can they be useful to your work in the lab?&lt;/li&gt;
  &lt;li&gt;Create your first repository and practice the workflow of committing and pulling/pushing files.&lt;/li&gt;
  &lt;li&gt;Talk about some of the more advanced features of tools: why might you need to use them, particularly in terms of collaboration.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;git-and-github&quot;&gt;Git and Github&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Git&lt;/strong&gt; is a open source version control system, a tool that manages the changes of files in a project. Each revision is associated with a timestamp and the person making the change, making it easy to revert back to previous versions of the file if you make a mistake or accidentally break something.  Version control systems are most commonly used for source code management, most any type of files can be managed using a VCS. There are other version control systems on the market today (e.g., Mercurial, Subversion); Git is among the most popular today. Git operates by having distributed project ‘repositories’ that store your code.  Any server can be a Git server.  For example, the William’s lab server could be a Git server if we set it up that way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt; is a company that provides web-based Git repository hosting.  Github manages the servers used to store your project repositories, supports all features of git like branches, merges, and commits, as well as adding additional tools that facilitate project development and collaboration like wiki hosting and issue tracking. Github is a private company valued at over $2 billion with about 20 million &lt;a href=&quot;https://www.quora.com/How-many-users-does-GitHub-have&quot;&gt;users&lt;/a&gt; and nearly 40 million different projects. Before you trust it completely remember (1) it is a company that has motives other than just backing up your code and (2) if you choose to use GitHub and they do something bad or there is a systemwide failure, there are going to be 20 million other people just, if not more, as upset as your are. There are other git hosting platforms as well: Bitbucket, Gitlab, etc.&lt;/p&gt;

&lt;h3 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h3&gt;

&lt;h4 id=&quot;create-an-account&quot;&gt;Create an account&lt;/h4&gt;

&lt;p&gt;To use Github, you need to create an account.  The good news: basic accounts are free to everybody. People with a basic account can create an unlimited of public repositories (can bee seen by anyone). If you have an academic (.edu) email address, you can get an unlimited number of private repositories as well by signing up &lt;a href=&quot;https://education.github.com/pack&quot;&gt;here&lt;/a&gt;. Private repositories let you control who sees your code – it’s hidden from the general public.  In both public and private repositories you can control who changes your code.&lt;/p&gt;

&lt;p&gt;To create an account, go to &lt;a href=&quot;https://github.com/join&quot;&gt;https://github.com/join&lt;/a&gt; and fill out the fields.  Make sure to use you &lt;code class=&quot;highlighter-rouge&quot;&gt;@wisc.edu&lt;/code&gt; email address.  Keep in mind that your user name will be your public name that identifies you on the platform and will be attached to everything you do there. Also, you’ll need to type in your username from time to time, so don’t make it &lt;em&gt;alongandannoyingtotypusername&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;getting-git-on-your-computer&quot;&gt;Getting Git on Your Computer&lt;/h4&gt;
&lt;p&gt;To use Git on your computer, you need to install it and configure it to work with Github servers. This content was borrowed from &lt;a href=&quot;https://help.github.com/articles/set-up-git/&quot;&gt;the Git Docs&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Download the latest version of Git from the &lt;a href=&quot;https://git-scm.com/downloads&quot;&gt;Git Website&lt;/a&gt;. Make sure to get the distribution that corresponds to your operating system, i.e., if you have a Windows machine, don’t opt for the Mac download.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Install Git by following the instructions on the download.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open the &lt;code class=&quot;highlighter-rouge&quot;&gt;terminal&lt;/code&gt; on your computer.  In windows this is the &lt;code class=&quot;highlighter-rouge&quot;&gt;cmd&lt;/code&gt; shell.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tell Git your name so your changes will be attributed to your Github account:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git config --global user.name &lt;span class=&quot;s2&quot;&gt;&quot;YOUR NAME&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If the terminal does not recognize &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; as a command, you might not have installed it correctly, return to step 1.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tell Git your email address associated with your account.  Make sure to use the same email your signed up with, or things can get funky later on.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git config --global user.email &lt;span class=&quot;s2&quot;&gt;&quot;YOUR EMAIL ADDRESS&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;From here on out, the changes you make to your project and other people’s projects (if you’re collaborating) will be attributed to this email address and username.&lt;/p&gt;

&lt;h4 id=&quot;create-a-repository&quot;&gt;Create a repository&lt;/h4&gt;

&lt;p&gt;A repository is where your project lives. From the github &lt;a href=&quot;https://help.github.com/articles/github-glossary/#repository&quot;&gt;documentation&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“A repository is the most basic element of GitHub. They’re easiest to imagine as a project’s folder. A repository contains all of
  the project files (including documentation), and stores each file’s revision history. Repositories can have multiple collaborators and can be either public or private.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Github is not just a way to version control your code, it’s also a way to organize your work.  A repository should contain all of the code, files, documentation, etc needed for a project.  For example, I have one repository for my thesis, a separate one for &lt;a href=&quot;https://github.com/scottsfarley93/IceAgeMapper&quot;&gt;Ice Age Mapper&lt;/a&gt;, and another one for &lt;a href=&quot;https://github.com/scottsfarley93/scottsfarley93.github.io&quot;&gt;my blog posts&lt;/a&gt;. Each one has everything it needs for someone to download it and work with that project on their own computer, but nothing extra that might span over to another project.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;There are several ways to create a new repository:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Click on the ‘+’ button in the top right of any page, then select ‘New repository’.
 &lt;img src=&quot;/assets/github/create_2.png&quot; alt=&quot;Create1&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;On your home page, click the green ‘New’ button.
 &lt;img src=&quot;/assets/github/create_1.png&quot; alt=&quot;Create2&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Go to &lt;a href=&quot;https://github.com/new&quot;&gt;https://github.com/new&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Create a repository from your desktop.  This is a bit more involved.  See &lt;a href=&quot;https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Give your repository a name.  This name should be short, yet descriptive of your project.  You’ll have to type this name every time you want to access your project so short and sweet is ideal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add a description of the project.  This is a useful place to describe a bit more about your project – particularly important if you’re sharing your repo with other collaborators.  Click ‘Initialize this project with a README’. This will give you a &lt;code class=&quot;highlighter-rouge&quot;&gt;README.md&lt;/code&gt; file where you can describe your project in detail. You can populate this file with more details later.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choose whether to make your project public or private.  I would recommend public repos for most projects, unless they contain sensitive code/information, stuff for publication, or other content you don’t want everyone in the world to see.  A major part of working with code and on github in particular is collaboration and openness with projects. I currently have only two private repositories: (1) labs and solutions for Geography 378 and (2) my thesis, because it contains details about the database I’m pushing data into.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choose whether to add a &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file and/or a license file.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file tells the git system not to track specific file types.  A &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file can be configured to tell git not to track any type of file.  If you click the box at this point, it will pre-populate a gitignore for you with a list of files that are often ignored when working in a particular language.  For example, if you’re working in R, selecting an &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; will stop the tracking of .Rdata (your workspace), .Rhistory (your command history), Rstudio files, caches, tempory directories, and files associated with converting markdown to documentation.  See &lt;a href=&quot;https://github.com/github/gitignore&quot;&gt;this repository&lt;/a&gt; for a list of all the options and the files they ignore. You can add to this file later in the course of your project too.&lt;/p&gt;

    &lt;p&gt;A license file tells users of your code how it may be used.  If you’ve chosen to use a public repository, you’ve already committed to some degree of openness in content use. The license file will discriminate between things like commercial use, totally unrestricted use, etc. Github produced &lt;a href=&quot;http://choosealicense.com/&quot;&gt;this website&lt;/a&gt; if you need to choose a license but don’t know which one you need.  I don’t usually license my repos.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;setting-up-your-repository&quot;&gt;Setting up your repository&lt;/h4&gt;

&lt;p&gt;Now you have a new repository, it’s currently living only on the Github servers. It’s time to set it up on your computer. There are a couple ways to do this.  I’ll focus on using command line, because it provides the most flexibility, though it can be a little intimidating at first. Make sure you have git installed on your computer before starting. I have a mac, so commands here are POSIX (mac and linux) specific, but the same concepts apply to git shell in windows. Kevin uses git on windows, so he can help answer your questions here.&lt;/p&gt;

&lt;p&gt;We’re first going to ‘clone’ the repository into a folder of your choosing.  It will create a new folder, so cloning into your documents folder will make a folder in &lt;code class=&quot;highlighter-rouge&quot;&gt;documents/yourRepo&lt;/code&gt;. Cloning will load all of the files contained in the github repository onto your local machine. The clone URL is the URL of your project, e.g., &lt;a href=&quot;https://github.com/scottsfarley93/IceAgeMapper&quot;&gt;https://github.com/scottsfarley93/IceAgeMapper&lt;/a&gt;. To change folders in the command line shell, you will use the &lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt; command.  &lt;code class=&quot;highlighter-rouge&quot;&gt;cd ..&lt;/code&gt; navigates up in the directory structure, &lt;code class=&quot;highlighter-rouge&quot;&gt;cd [directoryName]&lt;/code&gt; switches into a directory. &lt;code class=&quot;highlighter-rouge&quot;&gt;ls&lt;/code&gt; lists the content of a directory, and can be helpful to figure out where you are.  Once you’ve found a place you want to clone into, clone the repository:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git clone https://github.com/[YOUR USERNAME]/[YOUR REPOSITORY NAME]
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Your repository is now set up as a repository folder on your computer and any changes we make &lt;em&gt;inside&lt;/em&gt; of that folder, will be tracked.  At specific points in time, we’ll tell Git that we’ve made a suitable number of changes and want to mark that as a waypoint in our project history, using commits.&lt;/p&gt;

&lt;h4 id=&quot;making-changes&quot;&gt;Making Changes&lt;/h4&gt;
&lt;p&gt;Github tracks things called Diffs – a record of what was added, deleted, or changed inside of every file in your repo, a lot like the track changes functionality in MS Word. Git is different that Word, though, because you can revert back to previous timestamps. To mark a specific timestamp as a place you might want to come back to, you &lt;strong&gt;commit&lt;/strong&gt; your project. Committing is kind of like adding a waypoint to your project – each file is marked at its current configuration at this point in time. First, we’re going to tell Git which files to add to the commit, then we’re going to do the commit, identifying it by a short message. Finally, we will push our local changes to the Github server. Before committing, open a terminal and navigate (using &lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt;) to inside of your repository folder.&lt;/p&gt;

&lt;p&gt;1. Add files to the commit:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git add --all
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It is possible to only add specific files to your commit, but I nearly always just tell git to track all of the files inside of my project repository. If you don’t want specific files to be tracked, you can add them to your &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;2. Make the commit&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git commit -m &lt;span class=&quot;s2&quot;&gt;&quot;THIS IS A COMMIT MESSAGE&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Your commit messages should be short, easy to identify messages about what you changed since your last commit.  Traditional commit messages are things like “Added feature XYZ”, “Fixed bug ABC”, “Changed QRS to MNO”, etc. It is important to clearly identify your commits so that if you need to revert back to a previous version, you know which version to use.&lt;/p&gt;

&lt;p&gt;3. Push your changes to the Github server&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git push &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;origin branch]
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This step takes all of the commits that we’ve made on the local computer and pushes them out to the Github server.  It is not absolutely necessary to push after every commit, but I usually do – that way it’s backed up remotely on the Github server. Depending on your project configuration and if you’re working on a branch that’s not the default (more on that later) you may need to add the &lt;code class=&quot;highlighter-rouge&quot;&gt;origin branch&lt;/code&gt; at the end of the line to properly push.&lt;/p&gt;

&lt;h4 id=&quot;making-changes-an-example&quot;&gt;Making Changes: An Example&lt;/h4&gt;
&lt;p&gt;To show you how Github keeps track of things, we’re going to work through a short example.&lt;/p&gt;

&lt;p&gt;1. Open R or RStudio. Open a new RScript. We’re first going to define a variable &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; equal to 1. If you don’t like R, just open a text file and play along using your own short content examples.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;## define x to be one
&lt;/span&gt;    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Save the file into your project repo folder.&lt;/p&gt;

&lt;p&gt;2. Let’s commit the file now.  It’s traditional on the first commit you make on your project to give it the message &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Initial Commit&quot;&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;    git add --all
    git commit -m &lt;span class=&quot;s2&quot;&gt;&quot;Initial Commit&quot;&lt;/span&gt;
    git push
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;    Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git commit -m &lt;span class=&quot;s2&quot;&gt;&quot;Initial Commit&quot;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;master 6fef42f] Initial Commit
   1 file changed, 1 insertion&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;+&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   create mode 100644 myTrackedFile.R
  Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git push
  warning: push.default is &lt;span class=&quot;nb&quot;&gt;unset&lt;/span&gt;; its implicit value has changed &lt;span class=&quot;k&quot;&gt;in
  &lt;/span&gt;Git 2.0 from &lt;span class=&quot;s1&quot;&gt;&#39;matching&#39;&lt;/span&gt; to &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt;. To squelch this message
  and maintain the traditional behavior, use:

    git config --global push.default matching

  To squelch this message and adopt the new behavior now, use:

    git config --global push.default simple

  When push.default is &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;to &lt;span class=&quot;s1&quot;&gt;&#39;matching&#39;&lt;/span&gt;, git will push &lt;span class=&quot;nb&quot;&gt;local &lt;/span&gt;branches
  to the remote branches that already exist with the same name.

  Since Git 2.0, Git defaults to the more conservative &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt;
  behavior, which only pushes the current branch to the corresponding
  remote branch that &lt;span class=&quot;s1&quot;&gt;&#39;git pull&#39;&lt;/span&gt; uses to update the current branch.

  See &lt;span class=&quot;s1&quot;&gt;&#39;git help config&#39;&lt;/span&gt; and search &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;push.default&#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;further information.
  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;the &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt; mode was introduced &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;Git 1.7.11. Use the similar mode
  &lt;span class=&quot;s1&quot;&gt;&#39;current&#39;&lt;/span&gt; instead of &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;you sometimes use older versions of Git&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

  Counting objects: 3, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  Delta compression using up to 4 threads.
  Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2/2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  Writing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3/3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, 327 bytes | 0 bytes/s, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  Total 3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  To https://github.com/scottsfarley93/Williams-Lab-Github-Practical
     80ddf92..6fef42f  master -&amp;gt; master
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you see a message that informs you that your commit was &lt;strong&gt;&lt;em&gt;rejected&lt;/em&gt;&lt;/strong&gt;, try pulling remote changes first (next section).&lt;/p&gt;

&lt;p&gt;3.  We’re really hard working grad students, so now we’ve updated our R file. Specifically, we no longer think that &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is 1, but rather 2. Also, we’ve discovered a variable &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;, and it’s value is 5. In our R file, we’re going to change the value of &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; and add the statement defining &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Save the file and commit again.&lt;/p&gt;

&lt;p&gt;4. Now, go to &lt;a href=&quot;http://github.com&quot;&gt;github.com&lt;/a&gt; and let’s look at the changes that github keeps track of and how we might use the file history.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a. Go to your project repository page. Notice that your R file that we’ve been working on appears there now.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b. Click on the file’s name to see more details about it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/github_file.png&quot; alt=&quot;GithubFile&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;c. Click the &lt;code class=&quot;highlighter-rouge&quot;&gt;history&lt;/code&gt; button to show a summary of the commits that have been placed on that file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/github_history.png&quot; alt=&quot;github_history&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;d. Notice that there are two commits on this file: the first one, when we first created it and the second one, after we added the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;.  Click on a commit message to see what was changed during that commit. Click on the most recent commit message.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/github_commit.png&quot; alt=&quot;github_commit&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;e. Here is where we can see what Github is keeping track of. In the red, we see things that we removed from the file in the commit.  In the green are lines that were added to the file during that commit. Each file is identified by a hash – the long string of letters and numbers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;f. To look at the raw files at the point when the commit was made, go back one page to the listing of commits and click on the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt; &amp;gt;&lt;/code&gt; button to show the file at that point in time.  You will notice when you do that is not project in its current state, but shows the state of project when you made that commit.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/git_older.png&quot; alt=&quot;github_older&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pulling-remote-files&quot;&gt;Pulling remote files&lt;/h4&gt;
&lt;p&gt;One of the most powerful aspects of Github is its ability to unify your workflow across different computers. For example, Kevin is using it to merge code changes across the desktop in his office and his laptop. If you’re using it in this way, you’re going to need to update your working copy (e.g., on your laptop) with other changes you made previously (e.g., on the desktop).  To do that, we’re going to &lt;em&gt;sync&lt;/em&gt; or &lt;em&gt;pull&lt;/em&gt; changes from the Github server. &lt;strong&gt;This is different from a pull request.&lt;/strong&gt;  We’ll touch on pull requests later. The terminology here is one of the most difficult parts of github, because it conflicts with another aspect of the platform and it is different between the command line and the desktop app (if you’ve used that). Nonetheless, it’s actually pretty easy to do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git pull
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point a number of things might happen:&lt;/p&gt;

&lt;p&gt;1.  Everything you have on your local machine already reflects the most recent changes; there were not remote changes to fetch:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git pull
  Already up-to-date.
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;2.  You have remote changes, but it can be done without conflicts:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git pull
  remote: Counting objects: 3, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2/2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Total 3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, pack-reused 0
  Unpacking objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3/3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  From https://github.com/scottsfarley93/Williams-Lab-Github-Practical
     d988f65..1b21b38  master     -&amp;gt; origin/master
  Updating d988f65..1b21b38
  Fast-forward
   myOtherTrackedFile | 1 +
   1 file changed, 1 insertion&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;+&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   create mode 100644 myOtherTrackedFile
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3.  You have remote changes and they conflict with previous changes you have on your computer that you need to merge manually:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  remote: Counting objects: 3, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2/2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Total 3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, pack-reused 0
  Unpacking objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3/3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  From https://github.com/scottsfarley93/Williams-Lab-Github-Practical
     7ebd809..ecf9d38  master     -&amp;gt; origin/master
  Auto-merging myTrackedFile.R
  CONFLICT &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;content&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: Merge conflict &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;myTrackedFile.R
  Automatic merge failed; fix conflicts and &lt;span class=&quot;k&quot;&gt;then &lt;/span&gt;commit the result.
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this case, you will need to determine where your changes conflict with each other, fix them, make a new commit, and try syncing again.  If you navigate to your files, you will find something that looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEAD&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;=======&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecf9d381b84656db7f08c480d5989145a5d79b7f&lt;/span&gt;

  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This shows you where the conflict is.  Delete the lines you don’t want, removing the commit hash strings, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; characters as well.  Try committing again. This is a pain in the ass.  Try to avoid having this happen.&lt;/p&gt;

&lt;p&gt;4.  You have remote changes and they conflict with previous changes on your copy, but they can be merged automatically:&lt;/p&gt;

&lt;p&gt;In this case, you will be able to merge automatically, but you will need to add a “merge message” – similar to a commit message.  A text editor will open in your terminal where you can enter this message.  Depending on your computer, one of at least two different editors will open.  On a mac, either &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;nano&lt;/code&gt; will open to do the changes. To use &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt;, type your commit message then &lt;kbd&gt;:&lt;/kbd&gt; + &lt;kbd&gt;q&lt;/kbd&gt; + &lt;kbd&gt;Return/Enter&lt;/kbd&gt;.  To use nano, type your message, then &lt;kbd&gt;Control&lt;/kbd&gt; + &lt;kbd&gt;o&lt;/kbd&gt; to save your edits and &lt;kbd&gt;Control&lt;/kbd&gt; + &lt;kbd&gt;x&lt;/kbd&gt; to exit the editor. On windows, you’re on your own. You can set your default editor using command line, which would be helpful.&lt;/p&gt;

&lt;p&gt;The complexity here is for a reason – to be able to test code that comes from your collaborators before you merge it into your copy.  If you’re just working on your own project, these changes are always your own, so this merging stuff seems like overkill. But, when you’re working with one or more other people, you can’t be sure that something won’t break when you include their code.&lt;/p&gt;

&lt;h4 id=&quot;pulling-remote-files-an-example&quot;&gt;Pulling Remote Files: An Example&lt;/h4&gt;
&lt;p&gt;Github.com allows you to edit your files online, so we’re going to use that as a proxy for having two computers to work with. Note: I wouldn’t really recommend using the online platform to edit your files in (though I do it all the time).  If you’re going to make changes, just pull your repo and make changes on your local computer, or you risk having a lot of merge conflicts.&lt;/p&gt;

&lt;p&gt;1.  Go to your project repo page on github.com.&lt;/p&gt;

&lt;p&gt;2.  Create a new file, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;Create new file&lt;/code&gt; button.&lt;/p&gt;

&lt;p&gt;3.  Give it a name and remember to add the file extension.&lt;/p&gt;

&lt;p&gt;4.  Type some stuff.  For this example, I’m ‘using’ R, so I’ll define a new variable &lt;code class=&quot;highlighter-rouge&quot;&gt;z&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;  &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10238&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;5.  Save/commit the file. Github will automatically give you a commit message of “[Create/Update] [YOUR FILENAME]” if you’re doing changes on Github.com, so feel free to use that.&lt;/p&gt;

&lt;p&gt;6.  Return to your terminal, making sure you’re in the right project directory.&lt;/p&gt;

&lt;p&gt;7.  Pull the changes you made remotely.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git pull
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;8.  Look in the project folder.  Note that the file your created online is now stored on your local computer, the same as any other file in your project.&lt;/p&gt;

&lt;h4 id=&quot;advanced-github-features-and-scenarios&quot;&gt;Advanced Github: Features and Scenarios&lt;/h4&gt;

&lt;p&gt;One of the most confusing and intimidating aspects of Git/Github is the complicated and diverse set of features and terminology that goes along with the core concepts introduced above. Complicated and diverse, but powerful, especially when you work on long projects with multiple collaborators. Here, we will take a look at some of these concepts, when you might need them, and how they might be useful to your work.  By time time you need these constructs, you will be an expert in committing and syncing with remote repositories, so reading the docs for these things shouldn’t be as challenging. Content in the following section makes heavy use of the Github docs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Branches&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You’ve been working super hard all week on some super cool code stuff.  On Friday afternoon, you finally get it to work! You’re super psyched.  But, your weekly meeting with Jack is not until the following Wednesday.  Being the productive grad student that you are, you want to keep working on it over the weekend, adding a new feature.  But you also want to impress Jack with your recent developments and don’t want those to be compromised by having a feature that might break those whole thing all together. What to do?&lt;/p&gt;

    &lt;p&gt;Enter &lt;em&gt;branches&lt;/em&gt;. Branches are parallel projects, isolated from each other, on which you can develop new features, without fear that you will forever break your main code.  From the git documentation: “Branching means you diverge from the main line of development and continue to do work without messing with that main line.”  Git encourages branching early and often, even multiple times in a day.&lt;/p&gt;

    &lt;p&gt;So, in the case of our scenario, we are able to have a branch &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; that contains our version that we will show Jack.  On a &lt;em&gt;separate branch&lt;/em&gt; we will have our new feature.  We can develop on our new feature without messing up the master branch. When it comes time to meet with Jack, we will just sync to the latest commit of the master branch.  After we’ve perfected our new feature, we can merge it into the master branch and make it part of the main project.&lt;/p&gt;

    &lt;p&gt;Committing and syncing on feature branches goes the exact same way as committing and syncing on the master branch.  In fact, the master branch isn’t even a special branch, it’s just created by default when you make the repository, and you probably didn’t change it.  When you’re feature is ready to go prime time, you can merge the feature back into the master branch, even if you’ve made additional changes to the master branch since splitting off on the feature branch.  Look at &lt;a href=&quot;https://www.atlassian.com/git/tutorials/using-branches/git-merge&quot;&gt;this tutorial&lt;/a&gt; for an in-depth, accessible look at branching and merging.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://www.atlassian.com/git/images/tutorials/collaborating/using-branches/01.svg&quot;&gt;&lt;img src=&quot;https://www.atlassian.com/git/images/tutorials/collaborating/using-branches/01.svg&quot; alt=&quot;git_branches&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pull Requests&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You’re favorite R package is lacking a function that you think would be incredibly useful to you and other users of the package. You could submit an &lt;em&gt;issue&lt;/em&gt;, but you’re smart and figured out how to code the solution by yourself. You could just keep the code to yourself, but that’s no fun. Instead, you can make a &lt;em&gt;pull request&lt;/em&gt; to have your new code merged into the R package’s github repository, and thus the R package itself. In general, a pull request is a way to merge code into an existing repo.  If you just automatically merged it, it could break existing code and that would be bad. A pull request lets the owners of the repository test and discuss proposed changes with collaborators and add additional commits before the code is merged. Once everyone has agreed that the changes are good, the pull request can be approved and the changes will be merged.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Issues&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You’re working on a geovisualization platform for Neotoma data.  You talk with Jack, who talks with Eric Grimm and Simon and the other Neotoma folks, and you start getting emails about features that people would like or bugs that people have found while using your code.  You could just keep these emails, perhaps in a separate email folder, to remember what needs fixing or enhancing, or you could create a &lt;em&gt;Github issue&lt;/em&gt; to keep track of each feature and bug.  Issues let you make comments on feature requests and bug reports that are publicly viewable on your repo (if your repo is public). This is especially helpful if you’re working on a shared repository, or your code is being used as a library. You can also add labels and milestones to issues, reference specific commits, or assign issues to specific users. I use issues to track features and bugs even in small projects that I work on alone because they’re a good way to (1) have a to do list and (2) have conversations with myself about what I tried that worked (or not) in relation to a specific issue.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forks&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You really admire the work that Simon does in his &lt;a href=&quot;https://github.com/SimonGoring/NonAnalogues&quot;&gt;NonAnalogues&lt;/a&gt; and you want to use it in your work. You think that some serious modifications are necessary to the codebase to make it work for your specific application.  Simon is a pretty scary guy, though, so you don’t want to approach him about collaborating on your project, you just want to make the changes yourself.  You have two options: 1) you can download a .zip archive of all the files in their current form from Github and go from there or 2) you can make a &lt;em&gt;fork&lt;/em&gt; of his repository, which creates a new repository under your account, with all of the content from Simon’s project. In essence, a fork is a copy of a repository that allows you to freely experiment with changes without affecting the original project.  Github suggests that forks are most commonly used to either propose changes to someone else’s ideas or to use some else’s project as a starting point for your own idea.  Forks can be superior to downloading the raw files, because they let you stay in sync with the &lt;em&gt;upstream&lt;/em&gt; (original) repository. In this example, if Simon made a bug fix to his NonAnalogues repo at some point in the future, you could sync that change so that you didn’t need to fix that by yourself. Every public repository on Github can be forked by any user.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;collaboration-on-github&quot;&gt;Collaboration on Github&lt;/h4&gt;
&lt;p&gt;Github provides an excellent platform for collaborative work.  There are two models of collaboration that work well on Github. Now that we’ve talked about some of the more advanced features of Github, I think they will make more sense. The content here is adapted from the &lt;a href=&quot;http://cyber4paleo.github.io/resources/collaborating.html&quot;&gt;Cyber4Paleo (C4P)&lt;/a&gt; workshop resources from the Summer of 2016.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Fork and Pull
  This is the model that works best for large, many user projects require a stable branch for production (e.g., a code library). A user interested in collaborating on the project forks the repository, makes changes as she wants, then makes a pull request.  The maintainers of the project test the new code, make comments and changes as they see fit, and then merge the changes into the repository. This model allows for better project management, since and administrator can reject changes, ask for revisions, or decide the order in which changes are incorporated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared Repository
  This model is better suited to small projects with intimate/trustworthy teams. In this case, everyone works on the same repository without a forked repository as an intermediary. A user, who has already been vetted as a collaborator on the project, pulls the most recent version of the repository (using clone) onto her machine, makes changes, and then pushes back to the repository, immediate incorporating changes into the parent repository. This model only works with good communication.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With slightly larger teams, the shared repository starts to break down.  This can be overcome somewhat by using separate branches, which are then merged together into a master branch.  This allows users to work on their own features and changes, while still contributing to a single repo.&lt;/p&gt;
</description>
        <pubDate>Sat, 10 Sep 2016 09:22:52 -0500</pubDate>
        <link>/tutorial/2016/09/10/Williams-Lab-Github-Lesson.html</link>
        <guid isPermaLink="true">/tutorial/2016/09/10/Williams-Lab-Github-Lesson.html</guid>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Ecological Data: Is it &#39;Big Data&#39;?</title>
        <description>&lt;p&gt;We’ve all heard the term ‘Big Data’, though it’s often thrown around as a techy buzzword, along with others, like ‘The Cloud’, without a clear meaning.  In the Williams Lab, we’re working with datasets that are sometimes called ‘Big Data’ in talks by &lt;a href=&quot;https://twitter.com/iceageecologist&quot;&gt;@iceageecolgist&lt;/a&gt; and others, housed in databases like &lt;a href=&quot;http://neotomadb.org&quot;&gt;Neotoma&lt;/a&gt;, &lt;a href=&quot;http://gbif.org&quot;&gt;the Global Biodiversity Information Facility&lt;/a&gt;, and the &lt;a href=&quot;http://paleobiodb.org&quot;&gt;Paleobiology Database&lt;/a&gt;.  Today, I ask, what characteristics of our data make it ‘Big Data’?&lt;/p&gt;

&lt;h3 id=&quot;problem-and-scope&quot;&gt;Problem and Scope&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Question: Can ecological biodiversity data fit under the rubric of Big Data? If so, what are the characteristics that make it Big?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, let’s put a limit on the scope of the problem.  Ecology generally has many different subfields, each with their own data and data types.  Some of these may be particularly large, as in the case of ecological modelers, and some may be smaller.  For the sake of argument today, I’ll limit the discussion to ecological biodiversity data documenting &lt;em&gt;occurrences&lt;/em&gt;.   Each occurrence comes with metadata describing what species (typically, but could also be to another taxonomic grouping) was encountered, where it was encountered, and when it was encountered. This type of data is pervasive in the field, and can be used in a host of analyses, including modeling, climate change assessment, and hypotheses testing. Recently, there have been large international campaigns to aggregate these records into large, structured databases that facilitate global biodiversity syntheses.  Three that are commonly encountered are Neotoma (Quaternary), GBIF (modern and instrumental period), and PBDB (deep time).  Since PBDB’s &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; package was hard to use, I investigate the question today using data from Neotoma and GBIF.&lt;/p&gt;

&lt;h3 id=&quot;definitions-of-big-data&quot;&gt;Definitions of Big Data&lt;/h3&gt;
&lt;p&gt;There are two often-encountered, decidedly non-technical, designations of Big Data.  The first comes from Wikipedia&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Big data is a term for data sets that are so large or complex that traditional data processing applications are inadequate.&lt;/p&gt;

  &lt;p&gt;(Wikipedia)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is commonly seen in the marketing materials surrounding big computation and the cloud, though it’s really not a definition at all.  It doesn’t say much about what it is, just that ‘traditional’ means are not capable of processing it, pointing towards distributed computing, cloud computing, and other recent technological advances as its facilitator.  We do get a couple things from this definition though. We know that we’re looking at discrete data sets, partitioned, presumably, in a logical manner.  We’re looking for data sets that ‘traditional’ data processing applications are not feasible.  By using these words, ‘large’ and ‘traditional’, in particular, we can see that ‘Big Data’ is in the eye of the beholder, so to speak, and it depends on your tradition of data processing whether a new dataset is Big or not.  Guterman (2009) suggests, “for some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.”  From Guterman’s perspective, the focus is really on the number of bytes a dataset has, but as we’ll see in a minute, there can be other important factors that comprise a data set’s Bigness.&lt;/p&gt;

&lt;p&gt;The second defintion comes from Yang and Huang’s 2013 book Spatial Cloud Computing:
&amp;gt;Big Data refers to the four V’s: volume, velocity, veracity, and variety.
&amp;gt;
&amp;gt;(Yang and Huang, 2013)&lt;/p&gt;

&lt;p&gt;A varient of this definition can be traced back to an early IBM report on the topic, and can be seen in a variety of cheesy infographics, &lt;a href=&quot;http://www.ibmbigdatahub.com/infographic/four-vs-big-data&quot;&gt;like this one&lt;/a&gt;.  Yang and Huang go on to further describe the meaning of the four V’s, noting that “volume refers to the size of the data; velocity indicates that big data are sensitive to time, variety means big data comprise various types of data with complicated relationships, and veracity indicates the trustworthiness of the data” (p 276).  Here we get a bit more structure than the wikipedia definition gives us, and with the two together, we have a pretty good rubric on which to look at biodiversity datasets.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;h4 id=&quot;wikipedia&quot;&gt;Wikipedia&lt;/h4&gt;
&lt;p&gt;I argue that the very existence of complex relational databases, like GBIF, Neotoma and PBDB, suggest that biodiversity data do fall under the category of Big Data, as the traditional means of analyzing these data are possible anymore.  Of course, ‘complex’ in the context of the wikipedia statement typically refers to the preponderance of unstructured data, like videos and photos, and ‘large’ usually means too big to fit into a computer’s memory and/or storage drives.  From this perspective, our data is not complex, rather it’s stored in really organized relational tables, and fairly small (the entire Neotoma SQL dump can be downloaded at only 43MB).&lt;/p&gt;

&lt;p&gt;But, if we keep in mind that big data can mean different things to different people, then from our perspective in ecology, our data is Big. Consider the complexity of the relationships between different data records, for example. Figure 1 shows the Neotoma relational table structure, and the complicated web of relationships between each entity.  The data is both spatial and temporal, requiring these attributes, which are known to be messy (see “Veracity”), along with sample data and metadata.  Now, consider keeping track of this for tens of thousands (Neotoma) or hundreds of millions (GBIF) or records, among thousands of independent researchers, and we see why non-traditional techniques like these databases have been developed. Further developments, like APIs and R packages, are even more recent developments to further simplify the tasks of accessing, filtering and working with the datasets. No, ecological biodiversity data does not meet the scale and extent of YouTube, Twitter, or Amazon, but it does require new, custom built tools to store, analyze, and use.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.neotomadb.org/uploads/NeotomaDMD.pdf&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_ER.jpg&quot; alt=&quot;Neotoma_ER&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 1: Neotoma’s Relational Table Structure&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;volume&quot;&gt;Volume&lt;/h4&gt;
&lt;p&gt;Of the four V’s, the one that most comes to mind when considering what is, or is not, Big Data is volume: how much data is there?  As the quote from Guterman (2009) suggests, some experts consider this to be the only factor in determining what makes data Big. Our datasets are not on the scale of billions of hours of YouTube videos or hundreds of billions of Tweets, but the scale of biodiversity data has exploded in recent years, bringing it to a place where the volume alone is challenging to manage.&lt;/p&gt;

&lt;p&gt;Since the late 1990s, biodiversity databases have quickly and decisively increased the amount of data available to ecologists. Consider Figures 2 and 3, tracking the growth in collections of Neotoma and GBIF through time.  In 1990, only 2 of the records now stored in Neotoma were in digitized collections.  Today, there are over 14,000 datasets.  Each dataset is comprised of spatial and temporal metadata, along with one or more samples with data and associated metadata. The growth rate averages out to about 1.4 datasets every single day for over 26 years.  Considering the time, effort, and money that goes into working up a sediment core (or any of the other data types in Neotoma) this is a really impressive growth rate. For an interesting perspective on ecological Big Data’s reliance on blood, sweat, and tears, take a look at this &lt;a href=&quot;https://contemplativemammoth.com/2013/07/10/is-pollen-analysis-dead-paleoecology-in-the-era-of-big-data/&quot;&gt;Blog Post&lt;/a&gt; by former Williams Labber Jacquelyn Gill.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_Growth.png&quot; alt=&quot;Neotoma_Growth&quot; /&gt;
&lt;em&gt;Figure 2: Cumulative number of datasets in Neotoma&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The scale of GBIF is on an entirely different level than Neotoma (perhaps because some of the data gathering challenges faced in getting paleo data don’t apply as strongly to modern data collection). Today, GBIF houses digital records of well over 500 million observations, recorded specimens (both fossil and living), and occurrences noted in the scientific literature. GBIF’s records are largely comprised of museum collections, which allow their digital collection to date back to before 1900. The facility itself was introduced in 1999 and officially launched in 2001.  Since 2001, the facility’s holdings have grown nearly 300%, from about 180 million in 2001 to just shy of 614 million occurrence records today.  Managing 613+ million records and associated metadata, and comping with such a fast growth rate, is, without a doubt, a data management challenge worthy of Big Data classification.  Figure 3 shows the exponential growth in GBIF’s holdings since AD 1500, and Figure 4 is an interactive map showing the changes in spatial distribution of their observed data since the late 1800’s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/GBIF_Growth.png&quot; alt=&quot;GBIF_Growth&quot; /&gt;
&lt;em&gt;Figure 4: Exponential growth of occurrence records in GBIF&lt;/em&gt;&lt;/p&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/leaflet@0.7.7/dist/leaflet.css&quot; /&gt;

&lt;script src=&quot;https://unpkg.com/leaflet@0.7.7/dist/leaflet.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://code.jquery.com/jquery-3.1.0.slim.min.js&quot; integrity=&quot;sha256-cRpWjoSOw5KcyIOaZNo4i6fZ9tKPhYYb6i5T9RSVJG8=&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;div id=&quot;map&quot; style=&quot;height:500px;&quot;&gt;
&lt;/div&gt;
&lt;p&gt;1890&lt;input type=&quot;range&quot; min=&quot;1890&quot; max=&quot;2016&quot; step=&quot;1&quot; style=&quot;width:50%; display:inline-block; vertical-align:middle&quot; id=&quot;gbif_range&quot; /&gt;2016
&lt;script src=&quot;/assets/gbif_map.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Interactive – Spatial Distribution of GBIF Holdings Through Time&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;variety&quot;&gt;Variety&lt;/h4&gt;
&lt;p&gt;The second characteristic of Big Data in the four V’s framework is the Variety of the data, and its ‘various types with complicated relationships’ (Yang and Huang). Biodiversity data is highly diverse with many very complicated relationships and interrelationships.&lt;/p&gt;

&lt;p&gt;Neotoma’s holdings range from XRF measurements, to geochronologic data, to fossil vertebrates, to modern pollen surface samples.  In total, there are 23 dataset categories in the database, with more being added from time to time. Though it is structured similarly in the database tables, each of these data types comes from a different community of researchers, using different methods and instruments. Figure 5 shows the breakdown of dataset types in the database.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_types.png&quot; alt=&quot;Neotoma_Record_types&quot; /&gt;
&lt;em&gt;Figure 5: Dataset Type Breakdown of Neotoma’s Holdings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GBIF has 9 defined record type categories, including human observation, living specimen, literature review, and machine measurements.  As with the Neotoma dataset types, these are wildly different from each other.  A living specimen is clearly a totally different type of data to work with than something was derived from a literature review. Yet all of these types coexist together in these large biodiversity datasets. Figure 6 shows how GBIF’s records are distributed amongst these nine types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/gbif_types.png&quot; alt=&quot;GBIF&quot; /&gt;
&lt;em&gt;Figure 6: Dataset Type Breakdown of GBIF Holdings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To further add to the variety and complexity of our data, it is both spatial and temporal in nature, causing complicated interrelationships between data entities. 87.6 % of GBIF’s records are georeferenced to a real place in the world. 100% of Neotoma’s datasets have spatial information. In these databases, the spatial information is compounded by other fields that describe the location of the observation.  For example, Neotoma has fields describing the site where the fossil was found – it’s altitude, environment, area.  PBDB has extensive metadata for depositional environment, giving additional context to fossil occurrences.  GBIF often notes somewhat colloquial location descriptions in addition to geographic coordinates.   And, of course, there are the relationships between the spatial coordinates themselves – are these things in the same place? do they overlap?&lt;/p&gt;

&lt;p&gt;Managing data with a spatial component is nearly always more challenging than managing data without it. Figure 7 shows how the spatial locations of the datasets contained in Neotoma have changed through time.  Note the expansion in Europe and eastern Asia, and the lack of datasets in Africa.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/neotoma_spatial_dist.png&quot;&gt;&lt;img src=&quot;/assets/bigData/neotoma_spatial_dist.png&quot; alt=&quot;Neotoma_Maps&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 7: Spatial distribution of additions in Neotoma since 1990&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A final point on variety is that each record, though now cleanly structured and easily accessed as a record in a database, represents the work of an individual researcher.  The controlled vocabularies and organization policies enforced by the databases have helped to efficiently aggregate the data, however, nearly every record was collected, worked up, and published by a unique individual.  Figure 8 shows the number of datasets attributed to each PI in Neotoma.  Yes the names are too small to read.  The point, though, is that while a couple researchers have a very large number of datasets credited to them (John T Andrews has the most with 335), most have many fewer.  The median number of datasets contributed is 2, and the 3rd quartile value is just 7.  Each researcher will use different equipment, in a different way, call things different names, and generally just do things slightly differently – yielding a highly variable dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/Neotoma_PIs.png&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_PIs.png&quot; alt=&quot;GBIF&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 8: Neotoma dataset submissions by principle investigator&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;veracity&quot;&gt;Veracity&lt;/h4&gt;
&lt;p&gt;Ecological data has high levels of uncertainty associated with it.  Some can be estimated, like temporal and spatial uncertainty.  Others are less amenable to being quantified, for example inter-researcher identification differences, measurement errors, and data lost in the transition from field to lab to database. See &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0277379116300142#appsec1&quot;&gt;this paper&lt;/a&gt; for a Paleon project that used expert elicitation to quantify the differences between the dates assigned to European settlement horizon, a process they argue varies between sites, and depends on the “temporal density of pollen samples, time-averaging of sediments, the rapidity of forest clearance and landscape transformation, the pollen representation of dominant trees, which can dampen or amplify the ragweed signal, and expert knowledge of the region and the late-Holocene history of the site.” The raw data from the expert elicitation is included as supplementary information in their paper, and can be seen to vary pretty significantly between the four experts.&lt;/p&gt;

&lt;p&gt;Some information will be lost in the process of going from a field site through a lab workflow to being aggregated in the dataset.  Not all process details can be incorporated into database metadata fields, and probably more importantly, contextual details essential to proper interpretation of the data often gets lost on aggregation.&lt;/p&gt;

&lt;p&gt;Coincidentally, when I start working on my PhD here at UW, I’ll be working to tackle some of these uncertainty issues.&lt;/p&gt;

&lt;p&gt;To illustrate the veracity (or lack thereof) of the biodiversity data, let’s look at spatial coordinate uncertainty in GBIF and temporal uncertainty of chronological control points in Neotoma. The GBIF database, in addition to recording the geographic coordiantes of an occurrence, also includes a field for uncertainty in spatial location, though this field is optional.  I downloaded 10,000 records of the genus &lt;em&gt;Picea&lt;/em&gt;, of which over half did not include this field (though all were georeferenced).  This means that even if I am able include and propagate uncertainty in my models (as in Bayesian Hierarchical Models), I would be unable to do so really effectively, because few researchers even report this field. Of the 4,519 records that did report &lt;code class=&quot;highlighter-rouge&quot;&gt;coordinateUncertaintyInMeters&lt;/code&gt;, the average uncertainty was 305m (if you exclude zero, which seems reasonable to do). The maximum uncertainty in this dataset was 1,970m.  From this brief, and admittedly flawed, assessment, we can see there are some pretty serious problems with using the coordinates without considering their uncertainty first.  If, for example, you’re using 800m gridded climate model output to look at environmental covariates to species presence (which I do), a 300m uncertainty in species location could cause significant deviations due to gridcell mis-assignment, particularly in mountainous regions like the Western U.S.&lt;/p&gt;

&lt;p&gt;On the temporal side of things, we can do a similar assessment, this time using the Neotoma data.  Neotoma samples are assigned an age using age controls (like radiocarbon dates or varve counts) or an age model, which interpolates between the age controls. The age model issue is a challenging one, and there’s a lot of literature out there about it, as well as software to improve from simple linear models. Every age model is based on a set of age controls, which often have uncertainty associated with them.   Neotoma records an minimum and maximum age for each age control for each dataset.  Out of a sample of 32,341 age controls in the database, only 5,722 reported age uncertainty.  Some record types, like varves, can perhaps be assigned an uncertainty of zero, so we can safely ignore 2,830 more controls, leaving us with 2,892 that report values for minimum and maximum age. The summary statistics for these age controls suggest that the median age model tie point has a temporal uncertainty of 260.0 years. The 25% percentile is an uncertainty of 137.5 years and the 75% 751.2 years.  Using the mean of 260.0 years, I suggest that we can only identify down to ± 130 years of the actual date.  Considering sediment mixing, laboratory precision, and other processes at work, maybe this isn’t that big of a deal, but it definitely is something to be aware of and contributes to biodiversity data’s lack of absolute veracity.&lt;/p&gt;

&lt;h4 id=&quot;velocity&quot;&gt;Velocity&lt;/h4&gt;
&lt;p&gt;The final piece of the framework is the data’s velocity – how time sensitive is the data.  Data’s velocity important because high velocity data must be analyzed as a stream.  Tweets, for example, must be analyzed for trends as they are posted. Knowing the trending topics of two weeks ago might be interesting to me, but the real draw of a Big Data platform like twitter is that I can participate in the trending topics of &lt;em&gt;right now&lt;/em&gt;.  To do such an analysis, one must use sophisticated sampling techniques and algorithms to detect clusters and trends in real time, for example &lt;a href=&quot;http://jmlr.csail.mit.edu/proceedings/papers/v17/bifet11a/bifet11a.pdf&quot;&gt;this paper&lt;/a&gt;, which comments on sampling strategies used for trend detection.&lt;/p&gt;

&lt;p&gt;This is the one area where I would suggest that ecological biodiversity data is not Big Data.  Biodiversity analyses, like species distribution models, at least the ones I am familiar with, usually take between a few minutes and a few days to complete and are not especially time sensitive.  The rate of increase in data volume in both Neotoma and GBIF is not fast enough to invalidate the results from previous analyses.  Neotoma gets approximately 1.4 new datasets each day (1990-2016 average).  GBIF gets about 59,000 new occurrences each day (2000-2015 average).  Sure, that’s a lot of new datasets, but the likelihood you would actually use the new data in a given analysis is low, and the likelihood that its immediately inclusion into a new model would significantly change your conclusions is even lower.&lt;/p&gt;

&lt;p&gt;The velocity of data coming into the databases, particularly into GBIF, is staggering, no doubt about it.  Nonetheless, I don’t think it it warrants the use of specialized streaming algorithms for extracting information from the new data points.  I have not seen anyone attempt to do such a thing (though maybe this would be an interesting experiment?).  Moreover, there is little incentive to immediately analyze the data, because there is next to nothing to be gained from modeling biodiversity faster than you can report your results in publications.&lt;/p&gt;

&lt;h3 id=&quot;so-is-it&quot;&gt;So, is it?&lt;/h3&gt;
&lt;p&gt;Velocity notwithstanding, biodiversity occurrence data passes four of five facets of the Big Data, so I conclude that, &lt;strong&gt;yes, it is big data.&lt;/strong&gt; It requires specialized databases and software to interact with it, it has large numbers of records, it is extremely diverse, and it has high levels of uncertainty with which to deal.&lt;/p&gt;

&lt;p&gt;Looking forward, I suspect Big Data will continue to challenge those involved in synthetic research. Perhaps one of the most challenging aspects is the relatively short period of time in which these data became Big. Figures 9 and 10 show the annual increase in holdings for Neotoma (Fig. 9) and GBIF (Fig 10) through time (top) and the rate of change of annual increase (bottom). While Neotoma’s rate of increase as remained relatively steady through time (clear from the near-linear trend in Figure 2), GBIF’s rate shows a significant upward trend in the last several years.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/Neotoma_growth_diff.png&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_growth_diff.png&quot; alt=&quot;Neotoma_Delta&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 9: Neotoma holdings, Annual Change and Rate of Change of Annual Change&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/gif_growth_diff.png&quot;&gt;&lt;img src=&quot;/assets/bigData/gif_growth_diff.png&quot; alt=&quot;GBIF_Delta&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 10: GBIF holdings, Annual Change and Rate of Change of Annual Change&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Aug 2016 08:22:52 -0500</pubDate>
        <link>/research/paleo/2016/08/31/Big-Data-In-Ecology.html</link>
        <guid isPermaLink="true">/research/paleo/2016/08/31/Big-Data-In-Ecology.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleo</category>
        
      </item>
    
      <item>
        <title>Adding Shareable URLs to IceAgeMapper</title>
        <description>&lt;p&gt;One of the features Rob suggested I add to Ice Age Mapper during our last meeting was a dynamic url that would record the current state of the application, and could thus be shared between users. I took a stab at that last week, and got it working pretty well.  I thought it would be a lot of re-coding from the ground up, but it turns out that most of what I had written previously could be easily converted to load a URL string.  My application only generates a shareable URL when the user clicks the ‘Share’ button, but in theory, the app could easily be modified to generate a new URL each time an action was taken.  I think this would actually &lt;strong&gt;&lt;em&gt;Not&lt;/em&gt;&lt;/strong&gt; be a good idea, because it would mean there would be an entry in the user’s web history for each action they took inside of the application, meaning they would have to click the back button like a million times if they messed up.  Good to know support exists for that though.&lt;/p&gt;

&lt;p&gt;Another plus of designing a dynamic state URL is that it can be used to share the current configuration on Twitter, GooglePlus, by email, or other social media.  While not critical for our purposes, it seems like it’s never a bad thing to tap into social channels.&lt;/p&gt;

&lt;p&gt;There are two main parts of implementing a dynamic state url: generating and parsing.  The generating phase includes functions that get the current state of the system and translate them into a URL variable, and then string the URL variables together into a complete URL.  In the parsing phase, the URL variable parse (or not, if they don’t exist) and translate them into function calls to re-generate the desired state.  Before starting to code, make a list of the parts of the state you want to keep track of.  Do you want to keep track of every click made to get to a certain configuration or just the configuration itself?&lt;/p&gt;

&lt;p&gt;I decided I want to keep track of the following parts of the application state:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Taxon&lt;/em&gt;: the data that is currently being displayed, as returned from a Neotoma API call.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Map Center&lt;/em&gt;: Geographic center of current map view, as latitude and longitude.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Map Zoom&lt;/em&gt;: Zoom level of current map view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Minimum Year&lt;/em&gt;: Minimum (most recent) year in temporal view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Maximum Year&lt;/em&gt;: Maximum (most distant) year in temporal view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Panel Configuration&lt;/em&gt;: For each panel, is it open or closed?  Currently, there are three panels: Taxonomy, Site, and NicheViewer.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Layer Configuration&lt;/em&gt;:  For each layer, is it visible or hidden?  Currently, there are three layers: Ice Sheets, Sites, and Heatmap.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You’ll likely find that there are things you want to add to the state at a later time, but with the general framework, such additions should be really easy.&lt;/p&gt;

&lt;h3 id=&quot;part-1-generation&quot;&gt;Part 1: Generation&lt;/h3&gt;
&lt;p&gt;To generate the URL, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URI.js&lt;/code&gt; library.  This &lt;a href=&quot;https://medialize.github.io/URI.js/&quot;&gt;library&lt;/a&gt; makes it easy to parse, add to, and validate URL strings on the current window, or another window.  Generate a new URI for the current window location:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;URI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And then add query variables as needed, using:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;key&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next, and most intensive step in this phase to get the values of each state component at this point in time.  For some, like the leaflet map center and zoom, it will be easy to do this, because the leaflet map object already tracks these for you (&lt;code class=&quot;highlighter-rouge&quot;&gt;map.getCenter()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;map.getZoom()&lt;/code&gt;).  Depending on your coding style, you may already have pointers to some of the components, or you may need to devise a way of going to get the values.  Because I chose to only generate the new state URL once the user has requested it, we can write some functions to go get the values at the time they click the button. Mostly, though, I use a &lt;code class=&quot;highlighter-rouge&quot;&gt;globals&lt;/code&gt; object, that keeps references to a variety of important properties that I might want to access throughout my code.  I think this is a good compromise between having a ton of global variables floating around, and totally scoping the variables into functions.  Maybe I’m wrong, not sure…&lt;/p&gt;

&lt;p&gt;Anyways, for each of my state components, I go get it’s value, and then set it to our new &lt;code class=&quot;highlighter-rouge&quot;&gt;uri&lt;/code&gt; object.  Remember that the properties should be Boolean, String, or Numeric types, rather than object or something else that can’t be easily serialized into the URI.  This can be a little tricky, but it’s important so think about how you can make it work.  For example, if I want to populate a panel with data, I can’t easily serialize the data into the URI string.  Instead, I tell the URL that I do want to populate that panel, and I want that panel to automatically open.  Then I write re-write the panel function so that it can automatically call Neotoma and populate the details with the API call results. More on that in the next section.&lt;/p&gt;

&lt;p&gt;When the URI component contains all of your desired state components, you can get it’s value by calling &lt;code class=&quot;highlighter-rouge&quot;&gt;uri.toString()&lt;/code&gt;.  If you set &lt;code class=&quot;highlighter-rouge&quot;&gt;window.location.href=uri.toString()&lt;/code&gt; you will reload the page.  If that’s what you want, go for that.  In my case, I set a text box to the value of the &lt;code class=&quot;highlighter-rouge&quot;&gt;toString()&lt;/code&gt; method, which users can copy and paste if they want. In addition, I do add a history entry into the user’s browser history.  This is accomplished by:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;pushState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s about all there is on the generation side of things.  The more involved coding comes when trying to parse a share url.&lt;/p&gt;

&lt;h3 id=&quot;part-2-parsing&quot;&gt;Part 2: Parsing&lt;/h3&gt;
&lt;p&gt;Once you have a URL, you need to put in the infrastructure to generate the state that the URL calls for.  First though, you need to read the url string and parse it into its component parts.  To read the URL, I found that this function was super helpful (I borrowed it from &lt;a href=&quot;http://stackoverflow.com/questions/901115/how-can-i-get-query-string-values-in-javascript&quot;&gt;this StackOverflow post&lt;/a&gt;):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;[\[\]]&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;\\$&amp;amp;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;RegExp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[?&amp;amp;]&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;(=([^&amp;amp;#]*)|&amp;amp;|#|$)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;decodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\+&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;
&lt;p&gt;You can then get the query parameters from the URL string like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;queryVar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For each state component, I parsed the URI variable associated with it.  I also added some checks to make sure that if the query was not in the URI, the application wouldn’t crash, but would instead default to something smart.  For example, to get the currently displayed taxon:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;taxon&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//set the name in the search box&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#searchBar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toProperCase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;autoload&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once all of the query variables have been parsed, we need a way of translating the new state information into the actual application state.  I do this in two steps.  First, I have a function that does all of the parsing.  During the parsing, the global variable objects gets property values for the configuration (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;globals.taxon = &#39;Quercus&#39;&lt;/code&gt;).  Next, I call a load function, which is pretty much the same as what I had when I didn’t allow URL configuration, but instead of just setting the variables to &lt;code class=&quot;highlighter-rouge&quot;&gt;Null&lt;/code&gt; at the start, it checks to see if the property has already been set during the parsing phase.  This method works really well for components like map zoom and time extent.  However, it will not automatically load the data from Neotoma, because loading the data requires a button click to send an AJAX request to the Neotoma API.  Therefore, we add a &lt;code class=&quot;highlighter-rouge&quot;&gt;globals.autoload&lt;/code&gt; property, which automatically triggers a click on that button, if the necessary state configurtion variables (like taxon) are set in the URL.&lt;/p&gt;

&lt;h3 id=&quot;part-3--sharing-on-social-media&quot;&gt;Part 3:  Sharing on Social Media&lt;/h3&gt;
&lt;p&gt;One you’ve implemented the generating and parsing, and know that your share URL gives you a reliable application state representation, you can share the URL on twitter or other social media really easily.&lt;br /&gt;
#### Copying to the clipboard
While not social, you may wish to allow users to copy the link directly to their copy-paste clipboard.  I read some discussion of how this may be a bad idea for security.  I’m not sure – I added it anyways.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// create hidden text element, if it doesn&#39;t already exist&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#share-link&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;focus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//highlight the text element that contains the link&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;execCommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;copy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//do the copying&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//Boolean&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-on-twitter&quot;&gt;Sharing on Twitter&lt;/h4&gt;
&lt;p&gt;Twitter allows you to configure a link that pre-populates a tweet composer with message body, share url, hashtags, mentions, etc.  This was a little hard to get the hang of, and I still don’t think it’s quite right.  I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URI.js&lt;/code&gt; library again to generate this URL, and then set it to the &lt;code class=&quot;highlighter-rouge&quot;&gt;href&lt;/code&gt; property of a link.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;generateTwitterLink&lt;/code&gt; function is called right after the share URL is generated, so that it is available to the user if they choose to share on twitter. FYI: Even if you have a really long share url, it will only take up 22 characters of your tweet &lt;em&gt;if you are on a real server&lt;/em&gt;.  If you are on localhost, which isn’t a qualified domain, it will take up all of the characters, so might not work.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;generateTwitterLink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;URI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://twitter.com/share/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//base link&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//prepopulate with a URL&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Check out my Ice Age Map!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//prepopulate text&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hashtags&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;paleo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//generate the string from the object&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.twitter-share-button&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;href&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//set the link attribute so we actually use the dynamic URL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-with-email&quot;&gt;Sharing with Email&lt;/h4&gt;
&lt;p&gt;Perhaps the most likely way to share an application state for this application is by email, so I added a method that you can easily email the link out to your collaborators from inside the app. This is super easy, you just need to set the subject and body of the &lt;code class=&quot;highlighter-rouge&quot;&gt;mailto:&lt;/code&gt; string inside of the link &lt;code class=&quot;highlighter-rouge&quot;&gt;href&lt;/code&gt;. Since we don’t know who to send it to, we leave the &lt;code class=&quot;highlighter-rouge&quot;&gt;to:&lt;/code&gt; field blank.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;mailto:?to=&amp;amp;&quot;&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;subject=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;encodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//changes spaces to %20, etc&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&amp;amp;body=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#emailLink&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;href&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-on-google&quot;&gt;Sharing on Google+&lt;/h4&gt;
&lt;p&gt;Sharing on Google+, which I don’t know if anyone actually uses – I don’t–  was really, really easy. Assuming you have the required script included, you can have your sharing element be something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;div class=&quot;g-plus&quot; data-action=&quot;share&quot;&amp;gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then you can enable the sharing with your URL by setting the url data attribute inside of your javascript code, again, immediately after you generate the share URL.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.g-plus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;href&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sun, 21 Aug 2016 08:22:52 -0500</pubDate>
        <link>/research/paleo/2016/08/21/Adding-Shareable-URLs-To-IceAgeMapper.html</link>
        <guid isPermaLink="true">/research/paleo/2016/08/21/Adding-Shareable-URLs-To-IceAgeMapper.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleo</category>
        
      </item>
    
      <item>
        <title>Updating R on Debian Linux</title>
        <description>&lt;p&gt;Why the &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get&lt;/code&gt; package manager doesn’t contain the latest version of &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; automatically, I’m not sure. I recently realized I have been downloading a 2+ year old distribution for all of my SDM timing runs by running the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base&lt;/code&gt; command at the shell.  For several weeks, this was fine, but today the package &lt;code class=&quot;highlighter-rouge&quot;&gt;Rcpp&lt;/code&gt;, which wraps compiled C++ code in the R environment failed to compile.  I spent most of the afternoon trying to figure out what was going on.  I didn’t even occur to me that the  &lt;code class=&quot;highlighter-rouge&quot;&gt;r-base&lt;/code&gt; package I was using was the root cause.&lt;/p&gt;

&lt;p&gt;It is not easy to figure out how to update the core R package, but, like most things in linux, it comes down to a correctly ordered set of calls to a package manager.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; I am using a Debian 8 Jessie image, version v20160718&lt;/p&gt;

&lt;p&gt;###Steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get remove r-base&lt;/code&gt;.  Remove the old version of R.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo nano /etc/apt/sources.list&lt;/code&gt;.  This file holds all of the package repositories for &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Inside of it, copy and paste:
    &lt;pre&gt;
deb http://cran.rstudio.com/bin/linux/debian jessie-cran3/
&lt;/pre&gt;

    &lt;p&gt;This tells the manager to look in this repository for a copy of the R distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Save and close the text editor.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the shell, type:
  &lt;code&gt;
  gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9
  &lt;/code&gt;
  and then
  &lt;code&gt;
  gpg -a --export E084DAB9 | sudo apt-key add -
  &lt;/code&gt;
  What does this do? I’m not exactly sure, but I think it has to do with the package integrity checks down when downloading things from a package manager.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update&lt;/code&gt;.  Update the installed packages.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base&lt;/code&gt;.  Install the core R functionality, hopefully this time using the newest version.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base-dev&lt;/code&gt;.  Install the development headers to allow packages that are not in debian repositories.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point, you should have a newly updated R version.  You can check with R.version.  For me, this worked for updating from R version &lt;code class=&quot;highlighter-rouge&quot;&gt;3.0.1&lt;/code&gt; to R version &lt;code class=&quot;highlighter-rouge&quot;&gt;3.3.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have package install error, it’s definitely worth checking if an update in the r-base package could be responsible.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jul 2016 09:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/07/19/Updating-R-on-Debian.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/07/19/Updating-R-on-Debian.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Preliminary Thesis Results: Update 6/26</title>
        <description>&lt;p&gt;I’ve made it through 4,830 of the experiments I want to run for my thesis, so I’m taking this opportunity to reflect on the preliminary results that I have so far, visually check what I have so far, and make any necessary changes before doing the more expensive portion of the experiments.  So far, the results look okay, but definitely not what I expected.  The effect of the computing configuration on computing time seems to be minimal.  On the other hand, the effect of different experimental parameters is pretty significant.&lt;/p&gt;

&lt;h3 id=&quot;status&quot;&gt;Status&lt;/h3&gt;

&lt;h4 id=&quot;completion-statistics&quot;&gt;Completion Statistics&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Property&lt;/th&gt;
      &lt;th&gt;Number Recored&lt;/th&gt;
      &lt;th&gt;Percentage of Completed&lt;/th&gt;
      &lt;th&gt;Percentage of Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Completed&lt;/td&gt;
      &lt;td&gt;4830&lt;/td&gt;
      &lt;td&gt;92.18%&lt;/td&gt;
      &lt;td&gt;5.72%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Error&lt;/td&gt;
      &lt;td&gt;410&lt;/td&gt;
      &lt;td&gt;7.82%&lt;/td&gt;
      &lt;td&gt;0.05%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Not Started&lt;/td&gt;
      &lt;td&gt;59550&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;70.5 %&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Removed*&lt;/td&gt;
      &lt;td&gt;19680&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;23.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total&lt;/td&gt;
      &lt;td&gt;84470&lt;/td&gt;
      &lt;td&gt;–%&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Removed experiments are those that specify fitting the BRT model with 10,000 training examples, which takes too long to be practical.  Instead, these were replaced with the &lt;em&gt;nSensitivity&lt;/em&gt; experiment series, which tests the computing time sensitivity to different numbers of input points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/configs.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;inspection-of-results&quot;&gt;Inspection of Results&lt;/h3&gt;
&lt;p&gt;One of the main problems I’m having in interpreting the results is that there are four separate experimental variables, which makes it difficult to properly interpret the influence of only one variable.  Of course, I want to isolate the effect of the computing memory and virtual cores.&lt;/p&gt;

&lt;h4 id=&quot;influence-of-additional-cores&quot;&gt;Influence of Additional Cores&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/cores_totalTime.png&quot; alt=&quot;Cores&quot; /&gt;
If we plot vCPU vs. total time, there is no clear relationship.  The major spike at cores = 4 is due to the fact that I used a four core virtual machine to test the effect of different numbers of training examples.  These tests are not part of the experiments that I’m doing on every machine. If we remove these extraneous points, and treat them separately later, we can fit a linear model that shows a very slightly decreasing slope:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=136.103 - 4.548Cores&lt;/script&gt;

&lt;h4 id=&quot;influence-of-additional-memory&quot;&gt;Influence of Additional Memory&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/mem_totalTime.png&quot; alt=&quot;Memory&quot; /&gt;
The effect of adding additional memory is slightly more clearly linear and decreasing than the effect of adding addition CPU cores, although it is still not particularly steep.  The linear model here takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = 134.87 - 1.26GB&lt;/script&gt;

&lt;h4 id=&quot;influence-of-spatial-resolution&quot;&gt;Influence of Spatial Resolution&lt;/h4&gt;
&lt;p&gt;As expected, higher resolution outputs take longer to process than their lower resolution counterparts. Because increasing spatial resolution results in an exponential number of cells, a linear model is not particularly well suited to this application.  An exponentially decreasing relationship can be seen in the prediction time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/spatial_resolution.png&quot; alt=&quot;Spatial Resolution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The relationship between spatial resolution takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = 145.27 - 52.78degree&lt;/script&gt;

&lt;h4 id=&quot;influence-of-training-examples&quot;&gt;Influence of Training Examples&lt;/h4&gt;
&lt;p&gt;The clearest relationship in all of the experimental variables is between total model time and number of training examples.  This relationship is clearly monotonically increasing, perhaps at a rate slightly more than linear.  The linear fit for these two variables is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = -180.5674 + 0.3348trainingExample&lt;/script&gt;

&lt;p&gt;Nearly all of this additional time per training example comes from the time taken to fit the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/training_examples_totalTime.png&quot; alt=&quot;Training Examples&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fitting-a-generalized-linear-model&quot;&gt;Fitting a Generalized Linear Model&lt;/h3&gt;
&lt;p&gt;Using the &lt;code class=&quot;highlighter-rouge&quot;&gt;glm&lt;/code&gt; function in R, I fitted a generalized linear model to the data, using all four predictors.  Using all the predictors, the model takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = -176.0644 + 0.3343trainingExamples - 2.2136Cores + 2.2905GBMemory - 50.6004degree&lt;/script&gt;

&lt;p&gt;Using the Akaike Information Criterion (AIC) to evaluate the best model, I tried using different combinations of predictors. Using all four predictors, however, gives us the minimum AIC, so can be considered the best model out of all of the candidates.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AIC = 66800&lt;/script&gt;

&lt;h3 id=&quot;evaluating-the-accuracy-of-the-glm&quot;&gt;Evaluating the Accuracy of the GLM&lt;/h3&gt;
&lt;p&gt;Using an independent testing set of 200 random experiments, I used the glm above to predict the total time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/obs_pred.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, our GLM doesn’t do a great job at predicting the testing set to the observed values.  Perhaps I’m forgetting a variable…&lt;/p&gt;

&lt;p&gt;We can also plot out the errors between observed (‘true’) values and predicted values.  Looking at the summary statistics, it appears my model will slightly under predict the total execution time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary.png&quot; alt=&quot;SummaryStats&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;variance-within-cells&quot;&gt;Variance Within Cells&lt;/h3&gt;
&lt;p&gt;One of the things I was most worried about when starting this project was the within-cell variance that I would encounter due to internal computer variations and other concurrent processes.  Looking at the preliminary data, it appears that the variance within the cells does increase as the total experiment time increases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sd_mean.png&quot; alt=&quot;SD_Mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The linear model for standard deviation as a function of cell mean takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = 15.7146 + 0.05261x&lt;/script&gt;

&lt;p&gt;Where x is the cell mean.  Note: A cell is combination of cores, memory, training examples, and spatial resolution, and each cell is computed ten times.&lt;/p&gt;

&lt;h3 id=&quot;computed-configurations&quot;&gt;Computed Configurations&lt;/h3&gt;

&lt;p&gt;At this time I’ve computed:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;vCPU&lt;/th&gt;
      &lt;th&gt;Number Completed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1015&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;925&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;1455&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;GB Memory&lt;/th&gt;
      &lt;th&gt;Number Completed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;690&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;617&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;446&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;446&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;336&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;294&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;301&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sun, 26 Jun 2016 09:22:52 -0500</pubDate>
        <link>/research/visualization/2016/06/26/preliminary-thesis-results.html</link>
        <guid isPermaLink="true">/research/visualization/2016/06/26/preliminary-thesis-results.html</guid>
        
        
        <category>Research</category>
        
        <category>Visualization</category>
        
      </item>
    
      <item>
        <title>Building the Niche Database and Web Services</title>
        <description>&lt;p&gt;I’ve spent some time over the past couple of weeks building out the Niche API, a set of web services that enable you to get global climate model (GCM) simulated climate data for specific points in space and time.  For this project I’ve been mixing database design, backend web programming, and a bit of cloud computing.  It’s been a fun process, and is turning into what I think will be a very useful tool.  In this post, I put down a few thoughts about the decisions I made, the techniques I used, and the problems I faced.&lt;/p&gt;

&lt;h3 id=&quot;the-challenge&quot;&gt;The Challenge&lt;/h3&gt;
&lt;p&gt;Working with GCM output and other gridded climate products is always challenging. The data is large.  It’s usually in a format that is designed for its transportation rather than its communication (NetCDF). It may be in the wrong projection for the work you need to do.  You might need to resample the spatial resolution of the grid to fit with the other layers in your project.  If you’ve ever done work with these datasets, you’re probably familiar with these obstacles.  An additional challenge that we face in paleoclimate/paleoecological projects is the many different time periods that are included in each dataset. In a NetCDF, the different timestamps are typically shipped as different layers.  However, if you start to unpack these layers, you’ll quickly end up with a tangled mess of rasters describing different time periods.&lt;/p&gt;

&lt;p&gt;In many cases, we don’t need the whole gridded raster dataset for our work.  Maybe we need a small subset representing a study site, or very often, we just want to know the value of a variable at a discrete point in space and time.  In my work with species distribution models, I want to be able to ask the question: What was the precipitation (or maximum temperature, or annual mean temperature, etc) at the coordinates of a sample in the Neotoma database at the time when the sample was dated to.  Traditionally, I would need to manage all sorts of datasets, and use ArcMap or some other utility to get the datapoint I need.&lt;/p&gt;

&lt;h3 id=&quot;the-goal&quot;&gt;The Goal&lt;/h3&gt;
&lt;p&gt;My goal with this project is to build a web service that does the management and extraction of climate datasets automatically. My objects include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Store and manage gridded climate datasets in a way that preserves their metadata and makes them available via the internet&lt;/li&gt;
  &lt;li&gt;Be able to pass geographic coordinates and a calendar year and return an interpolated age for that space-time location&lt;/li&gt;
  &lt;li&gt;Be able to access the program scripts remotely through a web service&lt;/li&gt;
  &lt;li&gt;Make the platform generic enough to enable other users with other gridded datasets to contribute to the database&lt;/li&gt;
  &lt;li&gt;Maintain metadata on each dataset so that users can query for data by its attributes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My primary use case that I’m designing for is the &lt;a href=&quot;http://paleo.geography.wisc.edu&quot;&gt;NicheViewer&lt;/a&gt;, which plots samples in Neotoma on environmental axes.  This requires that the program be relatively fast (I’m still working on this) and return in a web-digestable format (JSON).  I’ve made some decision along the way that really reflect having NicheViewer as my target user, but at the &lt;a href=&quot;http://github.com/cyber4paleo&quot;&gt;C4P Hackathon&lt;/a&gt; in Boulder, CO this weekend, people showed significant enthusiasm for the project, and I’ll try to keep these other users in mind going forward.&lt;/p&gt;

&lt;h3 id=&quot;the-database&quot;&gt;The Database&lt;/h3&gt;
&lt;p&gt;Climate datasets are typically stored at NetCDF objects, a format which has been optimized for the sharing and storage of gridded data.  These files are self describing, with all the metadata needed to use them included in a header.  However, they’re difficult to extract data subsets from. Instead of relying on the NetCDF file type, I am going to the store the climate data as tables in a relational database.&lt;br /&gt;
Postgres is a relational database management system that has a great collection of spatial types included in the PostGIS extension.  With this extension, postgres has the ability to store large gridded datasets and to query them spatially.  It also includes the ability to store lines, polygons, multipolygons, and many other types of geometry if you want. Spatial queries are perfect for my task, because I can ask for the value of the raster at a latitude/longitude point.  I could also ask things like which cells are crossed by a line, or included in a polygon, but we’ll stick with points for now.&lt;/p&gt;

&lt;h4 id=&quot;raster-input-to-database&quot;&gt;Raster Input to Database&lt;/h4&gt;
&lt;p&gt;Getting rasters into the database is pretty easy, if you have them in a supported format (which NetCDF is not).  The workflow here was to first convert each band of the NetCDF (they’re not really bands, in this case they’re the discrete time/month/variable combinations) to a tiff file.  Then, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;raster2pgsql&lt;/code&gt; tool included with postgres, I’ll then convert each image file to a SQL table of the Binary Large Object (BLOb) type.  Finally, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;psql&lt;/code&gt; command shell, I can create a new table for this dataset in my dataset. I’ll typically do this in a python script that loops over all files in a directory.  Once I have a directory of tif files, I can do something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;subprocess&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;psycopg2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# connect to the database&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;connectString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dbname=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39; user=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39; host=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39; password=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psycopg2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connectString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# look through my local directory&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;basePath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/my/dir/of/files&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/my/dir/of/files&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# get the file name&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fullName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;basePath&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tableName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;RandomStringOfLettersAndNumbers&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# build the command&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;raster2pgsql &quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -s 4326&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##wgs 1984 coordinate system&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -d&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## overwrite this table name if it already exits&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -I&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##spatial index&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -C &quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##enforce constraints&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#command += &quot; -M &quot; ##vaccuum analyze&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rasterOnDisk&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -t 5x5 &quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##pixels per tile, the smaller the better&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tableName&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# run the command we&#39;ve just built&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Popen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PIPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;communicate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# run the SQL through the database connection&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# now the table is in the database&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;table-metadata&quot;&gt;Table Metadata&lt;/h4&gt;
&lt;p&gt;The only way a database like this can succeed is if there is sufficient metadata to allow users to search and filter based on a layer’s metadata: &lt;em&gt;who produced this layer? what are its units? what variable does it represent?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My database keeps track of raster layer metadata in a set of postgres tables. These tables are divided into two types: those describing the model and/or entity that produced the dataset and those describing the variable described by the dataset.  I think this is a natural basis for the data model, because GCM models can produce multiple variable types, but each raster layer can have only one variable and one model source.  Similarly, each the details of each variable (units, measurement type, etc) can be the same for different time stamps, and for different climate model sources.&lt;/p&gt;

&lt;p&gt;The metadata is tracked using foreign keys from a central index table.  The index table stores a pointer to the actual data layer table, which is identified by a 64-bit unique identifier.  The random string is used as a table name to ensure that table names will be uniquely mapped to their metadata, even if a lot of tables get added to the database.  The index table also stores layer-specific metadata, such as the spatial resolution of the raster and the time period being represented, which do not map on to sources or variables.  Finally, the table has columns for variables and sources that are further described in those respective tables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Source Metadata&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;sources&lt;/em&gt; metadata table keeps track of the model and entity that produced the data layer in the first place. The sources table includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Producer:&lt;/em&gt; The name of the modeling entity or research group that produced the layer set, (e.g. National Center for Atmospheric Research [NCAR])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Model:&lt;/em&gt; The name of the model that produced the layer set, (e.g. Community System Climate Model [CCSM])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ModelVersion:&lt;/em&gt; The version of the model that produced the layer set (e.g. 3.0),&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scenario:&lt;/em&gt; The forcing scenario under which the model was run (e.g., UN IPCC RCP8.5),&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ProductURL:&lt;/em&gt; The web address of the citation of the model, ideally a permanent &lt;a href=&quot;https://www.doi.org/&quot;&gt;doi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Variables Metadata&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;variables&lt;/em&gt; tables keep track of the thing that is being modeled.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;variableType:&lt;/em&gt; The climate variable being modeled, (e.g. Precipitation)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;variableUnits:&lt;/em&gt; The units in which the variable is measured (e.g. centimeters [cm])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;variablePeriod:&lt;/em&gt; The time frame over which the variable was measured, as represented by the layer (e.g., January [1])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;variablePeriodType:&lt;/em&gt; The type of measuring time frame (e.g. Month, Quarter, Annual)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;averagingPeriod:&lt;/em&gt;  The length of time over which the variable has been averaged, (e.g. 10)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;averagingPeriodType:&lt;/em&gt;  A description of the units of time describing the averaging period (e.g. Years)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties can be combined together such that there are many combinations, and detailed metadata can still be kept track of.  The structure is such that a SQL query can enable a machine API to query and search the variables layers.  Each layer must have all properties specified.&lt;/p&gt;

&lt;p&gt;One of the things I am looking at implementing is coming closer to the &lt;a href=&quot;http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html&quot;&gt;cf metadata standard&lt;/a&gt; that describes NetCDF files and already has a controlled vocabulary for climate layers.  I also want to change the variablePeriod implementation to reflect finer variations in represented time period, such as the difference between DJF and JFM.&lt;/p&gt;

&lt;h3 id=&quot;the-api&quot;&gt;The API&lt;/h3&gt;
&lt;p&gt;Once the database is in place, and has some data in it, we can enable programmatic access to its contents using an API. I have the both the database and the API running on a Google Compute Engine server running Debian Linux in the cloud.  This has proved really useful.  The API currently has way too many endpoints – you can add, modify, or delete nearly anything in the database – but that makes it really scalable if others want to start using it later on.  It’s been pretty RESTfully designed, and makes full use of the HTTP verbs.  The API is public, and the latest version of the documentation is &lt;a href=&quot;http://paleo.geography.wisc.edu/docs/&quot;&gt;here&lt;/a&gt;.  The full YAML declarations file is &lt;a href=&quot;http://paleo.geography.wisc.edu/docs/api_version_1_swagger.yaml&quot;&gt;here&lt;/a&gt;.  The API is &lt;a href=&quot;http://130.211.157.239:8080/&quot;&gt;served from here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;modeling-the-api&quot;&gt;Modeling the API&lt;/h4&gt;
&lt;p&gt;I wanted to be very methodical and document my progress in the development of this project, so I decided to use a modeling language that records the input and output properties of each endpoint.  I used the &lt;a href=&quot;http://editor.swagger.io/#/&quot;&gt;swagger&lt;/a&gt; modeling language and editor, which let me produce documentation and really think through my decisions before I started coding the application.  It’s designed to produce good documentation and is capable of generating client side code stubs for different languages which is kind of nifty.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;HTTP request&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /averagingTypes/{averagingTypeID}&lt;/td&gt;
      &lt;td&gt;Delete an averaging type using its averagingTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /averagingTypes/{averagingTypeID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific averaging type using its averagingTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /averagingTypes/{averagingTypeID}&lt;/td&gt;
      &lt;td&gt;Update details of a specific averaging type using its averagingTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /averagingTypes&lt;/td&gt;
      &lt;td&gt;Get a list of the averaging types in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /averagingTypes&lt;/td&gt;
      &lt;td&gt;Add a new averaging period to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /data&lt;/td&gt;
      &lt;td&gt;Get the value of one or more layers at a space-time location&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /layers&lt;/td&gt;
      &lt;td&gt;Get a list of the layers in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /layers/{layerID}&lt;/td&gt;
      &lt;td&gt;Delete a layer and its raster table using its layerID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /layers/{layerID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific layer using its layerID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /layers/{layerID}&lt;/td&gt;
      &lt;td&gt;Update a layer&#39;s details using its layerID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /layers&lt;/td&gt;
      &lt;td&gt;Add a layer to the databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /sources&lt;/td&gt;
      &lt;td&gt;Get a list of the data sources and models in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /sources&lt;/td&gt;
      &lt;td&gt;Add a new source to the database.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /sources/{sourceID}&lt;/td&gt;
      &lt;td&gt;Delete a source using its souce id&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /sources/{sourceID}&lt;/td&gt;
      &lt;td&gt;Get details about an specific source using its sourceID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /sources/{sourceID}&lt;/td&gt;
      &lt;td&gt;Update details about a specific source using its sourceID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableUnits&lt;/td&gt;
      &lt;td&gt;Get a list of variable units in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variableUnits&lt;/td&gt;
      &lt;td&gt;Add a new variable unit to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variableUnits/{variableUnitID}&lt;/td&gt;
      &lt;td&gt;Delete a variable unit using its database id&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableUnits/{variableUnitID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific variable unit instance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variableUnits/{variableUnitID}&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variablePeriodTypes&lt;/td&gt;
      &lt;td&gt;Get a list of the variable period types in the database.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variablePeriodTypes&lt;/td&gt;
      &lt;td&gt;Add a new variable period type to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variablePeriodTypes/{variablePeriodTypeID}&lt;/td&gt;
      &lt;td&gt;Delete an variable period instance using its variablePeriodTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variablePeriodTypes/{variablePeriodTypeID}&lt;/td&gt;
      &lt;td&gt;Get a details about a specific variable period type using its variablePeriodTypeID.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variablePeriodTypes/{variablePeriodTypeID}&lt;/td&gt;
      &lt;td&gt;Update the details of a specific variable period using its variablePeriodTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableTypes&lt;/td&gt;
      &lt;td&gt;Get a list of variable types in the database.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variableTypes&lt;/td&gt;
      &lt;td&gt;Add a new variable type to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variableTypes/{variableTypeID}&lt;/td&gt;
      &lt;td&gt;Delete a variable type instance using its variableID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableTypes/{variableTypeID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific variable type&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variableTypes/{variableTypeID}&lt;/td&gt;
      &lt;td&gt;Update details about a specific variable type in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variables&lt;/td&gt;
      &lt;td&gt;List Niche Variables&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variables&lt;/td&gt;
      &lt;td&gt;Add a new niche variable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variables/{variableID}&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variables/{variableID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific variable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variables/{variableID}&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;writing-the-api&quot;&gt;Writing the API&lt;/h4&gt;
&lt;p&gt;Once modeled, the API script was written in python using the &lt;a href=&quot;http://bottlepy.org/docs/dev/index.html&quot;&gt;bottle&lt;/a&gt; web framework and a paste python-based web server.  Eventually it will be migrated to a windows server housed in our lab, and run behind an apache web server as a CGI module.  Basically, all the API does is receive HTTP requests, route them to functions, then the functions make SQL queries, and return the results as JSON. Pretty simple.  As you can see, there are a lot of endpoints included on the API right now, but the most important one is the &lt;em&gt;data&lt;/em&gt; endpoint, where you can actually get data from the database.&lt;/p&gt;

&lt;p&gt;The server runs forever using the &lt;code class=&quot;highlighter-rouge&quot;&gt;supervisor&lt;/code&gt; linux module (see my post on long running linux programs).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Making a request&lt;/strong&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bottle&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hook&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;psycopg2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## database connector&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## for timestamp formatting&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JSONResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## base response class that returns the json fields I want&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;auto&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sucess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;auto&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Y-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;m-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;H:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;M:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;S&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;toJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## just report the class&#39;s fields as a dictionary that will get converted to real json when bottle returns it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__dict__&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;connectToDefaultDatabase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;Connect to the database using the settings configured in conf.txt&#39;&#39;&#39;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## read from the conf.txt file&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;conf.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;r&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hostname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;password&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;user&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;dbname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fieldname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&#39;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fieldname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hostname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;dbname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;password&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;user&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psycopg2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# now we can start making requests&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/someroute&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;doGetStuff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# do some get stuff here!&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# the query parameters are stored in request.query&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JSONResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#  make it serve&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;paste&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0.0.0.0&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Logging&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I want to be able to see who calls the API, which endpoints they call, and what kind of tech they’re using to call it.  I set up request logging on every request into a table called &lt;code class=&quot;highlighter-rouge&quot;&gt;call_log&lt;/code&gt;.  I accomplish this using a &lt;code class=&quot;highlighter-rouge&quot;&gt;hook&lt;/code&gt; which is fired before every request is passed on to its respective routing function, and basically just creates an apache style server log.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# this is the route that happens every time a request to anything is made&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@hook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;before_request&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_this_request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## this function deconstructs the request and puts it into a table&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;logRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connectToDefaultDatabase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;REQUEST_METHOD&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;server_protocol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SERVER_PROTOCOL&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user_agent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;HTTP_USER_AGENT&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;remote_ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;REMOTE_ADDR&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_string&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;INSERT INTO call_log VALUES(default, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resource)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(method)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(server_protocol)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(user_agent)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(remote_ip)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(args)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, default);&#39;&#39;&#39;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;method&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;server_protocol&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;server_protocol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;user_agent&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;remote_ip&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;resource&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;args&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;CORS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To make post requests from the server where NicheViewer lives, I also needed to enable CORS.  CORS is always a pain in the ass, but its handled in the API like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# define the headers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_allow_origin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;*&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_allow_methods&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;PUT, GET, POST, DELETE, OPTIONS&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_allow_headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Authorization, Origin, Accept, Content-Type, X-Requested-With&#39;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# attach them after every response&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@hook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;after_request&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enable_cors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;Add headers to enable CORS&#39;&#39;&#39;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Access-Control-Allow-Origin&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_allow_origin&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Access-Control-Allow-Methods&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_allow_methods&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Access-Control-Allow-Headers&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_allow_headers&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# if the browser sends an Options request, let it know that it&#39;s safe to send over a post request&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;OPTIONS&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;returnOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HTTPResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;__&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Interpolating the Ages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the biggest challenges for me was getting the correct query to interpolate variables between ages.  The code ended up looking like this (yes, it is convoluted):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#...import stuff and set up the connections&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#...collect query parameters&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#...get a new DB cursor&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;## fetch the table names that match our query&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# this query gets the tables and associated metadata that meet the user&#39;s query that have an age OLDER than the query year.&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;greaterThanQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
       SELECT
           &quot;tableName&quot;, rasterIndex.yearsBP, sources.sourceID, sources.model, sources.producer, sources.productVersion, variables.variableID,
           variables.variabledescription, variableTypes.variableType, variableUnits.variableUnitAbbreviation, variables.variablePeriod,
           variablePeriodTypes.variablePeriodType, variables.variableAveraging, averagingPeriodTypes.averagingPeriodType, sources.producturl
       from rasterIndex
       INNER JOIN variables on rasterIndex.variableID=variables.variableID
       INNER JOIN    sources on rasterIndex.sourceID = sources.sourceID
       INNER JOIN    variableTypes on variables.variableType = variableTypes.variableTypeID
       INNER JOIN    variableUnits on variables.variableUnits = variableUnits.variableUnitID
       INNER JOIN    averagingPeriodTypes on variables.variableAveragingType = averagingPeriodTypes.averagingPeriodTypeID
       INNER JOIN    variablePeriodTypes on variables.variablePeriodType = variablePeriodTypes.variablePeriodTypeID
       WHERE rasterIndex.recordID IN
           (SELECT MIN(recordID) FROM rasterIndex
                       WHERE 1 = 1
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableTypes.variableTypeAbbreviation) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variablePeriod )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variablePeriodTypes.variablePeriodType) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variableAveraging )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(averagingPeriodTypes.averagingPeriodType) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableUnits.variableUnitAbbreviation))
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variableID)
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.sourceID)
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = resolution)
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.model) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.producer) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.productVersion )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(scenario) )
                       AND (yearsBP &amp;gt;= &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(yearsBP)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s)
                       GROUP BY variableID
           )
       ORDER BY sourceID, variableID;
       &#39;&#39;&#39;&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriod&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableUnits&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableUnits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableID&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriod&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sourceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceProducer&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sourceProducer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelName&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelVersion&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelVersion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;resolution&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelScenario&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelScenario&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;yearsBP&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterThanQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## execute this query&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;greaterThanRows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## get and store the request&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;## THIS IS THE LESS THAN query&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;##This is the same query, but gets tables YOUNGER than the query age.&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lessThanQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
           SELECT
               &quot;tableName&quot;, rasterIndex.yearsBP, sources.sourceID, sources.model, sources.producer, sources.productVersion, variables.variableID,
               variables.variabledescription, variableTypes.variableType, variableUnits.variableUnitAbbreviation, variables.variablePeriod,
               variablePeriodTypes.variablePeriodType, variables.variableAveraging, averagingPeriodTypes.averagingPeriodType, sources.producturl
           from rasterIndex
           INNER JOIN variables on rasterIndex.variableID=variables.variableID
           INNER JOIN    sources on rasterIndex.sourceID = sources.sourceID
           INNER JOIN    variableTypes on variables.variableType = variableTypes.variableTypeID
           INNER JOIN    variableUnits on variables.variableUnits = variableUnits.variableUnitID
           INNER JOIN    averagingPeriodTypes on variables.variableAveragingType = averagingPeriodTypes.averagingPeriodTypeID
           INNER JOIN    variablePeriodTypes on variables.variablePeriodType = variablePeriodTypes.variablePeriodTypeID
           WHERE rasterIndex.recordID IN
               (SELECT MIN(recordID) FROM rasterIndex
                           WHERE 1 = 1
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableTypes.variableTypeAbbreviation) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variablePeriod )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variablePeriodTypes.variablePeriodType) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variableAveraging )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(averagingPeriodTypes.averagingPeriodType) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableUnits.variableUnitAbbreviation))
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variableID)
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.sourceID)
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = resolution)
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.model) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.producer) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.productVersion )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(scenario) )
                           AND (yearsBP &amp;lt;= &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(yearsBP)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s)
                           GROUP BY variableID
               )
           ORDER BY sourceID, variableID;
           &#39;&#39;&#39;&lt;/span&gt;

   &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriod&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableUnits&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableUnits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableID&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriod&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sourceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceProducer&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sourceProducer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelName&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelVersion&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelVersion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;resolution&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelScenario&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelScenario&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;yearsBP&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lessThanQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## execute&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lessThanRows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;## these are the fields that will be returned with the response&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;tableName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;yearsBP&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sourceID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Producer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ModelVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variableID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;s&quot;&gt;&quot;VariableDescription&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;VariableType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variableUnits&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variablePeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variablePeriodType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;averagingPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;averagingPeriodType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dataCitation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;## start building the response&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;## fetch the actual point data from each of the returned tables&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterThanRows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
       &lt;span class=&quot;c&quot;&gt;## the row order should match so we can interpolate between the sets of values&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterThanRows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;lesserRow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lessThanRows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## go get the actual values from the raster  &lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterYear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## for the higher value&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;SELECT ST_Value(rast,ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326)) FROM public.&#39;&#39;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
               WHERE ST_Intersects(rast, ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326));
           &#39;&#39;&#39;&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;latitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;longitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## for the lesser value&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserYear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;SELECT ST_Value(rast,ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326)) FROM public.&#39;&#39;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
               WHERE ST_Intersects(rast, ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326));
           &#39;&#39;&#39;&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;latitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;longitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## no results were returned, likely because point was outside of north america&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## no results were returned, likely because point was outside of north america&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## now do the Interpolation&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserYear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterYear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;predPoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Prediction point is &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predPoint&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interp1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
          &lt;span class=&quot;c&quot;&gt;## add metadata about the table&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;value&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## this is the actual point value&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;latitude&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;longitude&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;siteName&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;siteName&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;sampleID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampleID&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;siteID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;siteID&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;yearsBP&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## table doesn&#39;t exist, but record for table does exist --&amp;gt; oops&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;finally&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;## return the response&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JSONResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Linear interpolation between two nearest neighbors.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HTTPResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;api-responses&quot;&gt;API responses&lt;/h4&gt;

&lt;p&gt;The base response from the API includes these fields:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;status:&lt;/em&gt; HTTP status code, also included in the response header&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;timestamp:&lt;/em&gt; Time at which the response was minted by the server&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;message:&lt;/em&gt; A string produced by the server to specify an error, or note something of importance&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;success:&lt;/em&gt;  Boolean flag to indicate whether the API call was successful.  If success is &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; is empty, then the user can be sure that the call happened correctly, there just was no matching data found in the database.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;data:&lt;/em&gt;  An array of objects that meet the search criteria&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Status Code&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;200:&lt;/em&gt; Success!&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;201:&lt;/em&gt; The requested object already exists, so the script didn’t create a new one&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;404:&lt;/em&gt; Object not found (resource doesn’t exist)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;400:&lt;/em&gt; Required parameters for the method were not set&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;204:&lt;/em&gt; Object deleted&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I believe the current solution described here meets all of the goals I set out to accomplish.  It’s a little hacky in places, and could be more robust, but for the most part it does everything it has to without error.  It currently supports two front ends: the NicheViewer and an R package.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals, evaluated&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Store and manage gridded climate datasets in a way that preserves their metadata and makes them available via the internet&lt;/em&gt;: Storing the raster layers in a postgres database makes it available over the internet.  While the metadata becomes divorced from the actual data, storing it in other tables keeps it close by, so we can refer to it if we need it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Be able to pass geographic coordinates and a calendar year and return an interpolated age for that space-time location&lt;/em&gt;:  The API supports a SQL query that returns interpolated ages for spatial locations at a spatial location.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Be able to access the program scripts remotely through a web service:&lt;/em&gt;  The API supports RESTful querying of the data layers, so that user’s can get the data without installing software or downloading datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Make the platform generic enough to enable other users with other gridded datasets to contribute to the database:&lt;/em&gt;  The table structure is generic enough to support (I think) any 2-dimensional gridded climate model output.  3-dimensional (height) datasets will not fit well into the data model, nor will datasets that are not gridded.  Datasets that are not modeled, but perhaps show real observations, should be able to fit into the database, though it hasn’t been tested.  The API needs a little work to support direct data ingestion, but version 1 supports public curation of metadata.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Maintain metadata on each dataset so that users can query for data by its attributes:&lt;/em&gt; The table structure maintains sufficient metadata on the model and the variable that is represented.  It is stored in several tables which allows it to be included in a structured query.  Version 1 of the API allows clients to query on all facets of what makes up a layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Future Work&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I have several things I would like to continue adding to this application.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add more data layers, datasets, and models.&lt;/li&gt;
  &lt;li&gt;Improve API speed and robustness.&lt;/li&gt;
  &lt;li&gt;Let users query for other geometry types (lines, polygons, etc)&lt;/li&gt;
  &lt;li&gt;Improve metadata representations of time periods and variable types.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 26 Jun 2016 08:22:52 -0500</pubDate>
        <link>/research/paleoclimate/2016/06/26/the-niche-api-web-service.html</link>
        <guid isPermaLink="true">/research/paleoclimate/2016/06/26/the-niche-api-web-service.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleoclimate</category>
        
      </item>
    
      <item>
        <title>Automating my workflow: Building a (somewhat) distributed and (moderately) fault-tolerant system</title>
        <description>&lt;p&gt;I’ve made significant progress in getting a couple of Google’s computers to do my bidding (aka my thesis), in an automated way, so I thought I would share my experience setting up my cluster, and, specifically, the configuration of computing nodes and database/control nodes.  My setup draws on a bit on the design of larger systems like Hadoop, which create frameworks for massively large and distributed fault-tolerant systems.  In short, I have one Master Node that hosts a database and a control script, and a pool of compute nodes that are fault-tolerant and designed only for computing.  The compute nodes don’t have to know anything about the progress of the entire project and can handle being shut down mid-run, and the control node doesn’t have to know anything about the simulations being computed.&lt;/p&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Database:&lt;/em&gt; Needed to store the potentially large number of results obtained from the simulations&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Configurable Computers:&lt;/em&gt;  A main point of my thesis is that I can control the (virtual) hardware parameters of different computers and see how the simulation time responds.  Thus, I need a pool of computers with which I can control these parameters.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Automation:&lt;/em&gt; I’ve proposed the collection of ~84,400 different simulations on approximately 2,000 different hardware configurations. I don’t have the time or willpower to set all of these up manually, so I need a way to automate the process.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;API Access (optional):&lt;/em&gt; I am into API design and visualization, so I want to have an internet based method of getting the completion statistics for the project.  That way I can build a cool dashboard for the results and stuff, but this is last on the list of requirements.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;platform&quot;&gt;Platform:&lt;/h3&gt;
&lt;p&gt;I’ve decided to use the &lt;a href=&quot;http://cloud.google.com&quot;&gt;Google Cloud&lt;/a&gt; platform because of (a.) Its option for creating custom machine types that do not conform to predefined cloud computing servers and (b.) because of its free trial that allowed me to get a bunch more experiments in without the cost.  So far, my experience with google cloud has been overall positive, but not great.  There is an extensive amount of documentation on it, but it’s very dense and challenging if you don’t already have experience working within their frameworks.  There is only a limited amount of blogs/stackoverflows/etc to refer to if you encounter an error or problem.  On the other hand, there’s multiple ways of accessing your resources (console, REST api, command line), and several dashboards that give you a visualization of what’s happening.  So we’ve chosen the platform.&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;
&lt;p&gt;Based on the need to many, many different computing configurations and the automation/database needs, I think it makes sense to split the virtual servers I’ll have on the cloud into two groups. At any one time I will have one or many servers that will actually be doing the job of computing the species distribution models and assessing their time and accuracy (compute nodes).  At the same time, I will have one server that hosts the database and the API, starts and stops the computing instances, and cleans up the workspace when necessary (Master Node). This approach allows me to use Google’s preemptible instances, which cost much less, but have the potential to be dropped due to system demand at any time.&lt;/p&gt;

&lt;h3 id=&quot;top-level-infrastructure&quot;&gt;Top-Level Infrastructure&lt;/h3&gt;
&lt;p&gt;The top-level infrastructure is composed of two parts: the central database and the Master Node.  In truly distributed systems, the system would not need access to a centralized database, instead each compute node would be able to do its own thing.  This strategy seemed like overkill and difficult to implement for this project, so I kept a single centralized database – hope it doesn’t crash.  I started out using Google’s CloudSQL, which was really a pain to set up, a pain to get data into, and a pain to get data out of.  So I stopped using it. Instead I run a small virtual server (f1-micro) so that I can SSH into it and not need the &lt;a href=&quot;% post_url 2016-6-2-adventures-in-google-cloud-I %&quot;&gt;very confusing&lt;/a&gt; CloudSQL Proxy required for I/O into the database.&lt;/p&gt;

&lt;p&gt;Also hosted on this small server, but conceptually different is a set of scripts that make up the ‘brain’ of the experiment. One of these scripts runs standard SQL queries against the database to determine the current position within the pool of experiments I want to accomplish.  This is the basis for the API, developed in node.js, and also the basis for the script that controls the setup and teardown of the compute nodes.&lt;/p&gt;

&lt;h4 id=&quot;configuring-and-building-the-virtual-instances&quot;&gt;Configuring and building the virtual instances&lt;/h4&gt;
&lt;p&gt;The steps to building and configuring the pool of computing nodes takes follows this general process:
1.  The MasterNode.py script uses the (daemonized [always running]) node.js web backend to query the database to ask “What experiments have not yet been marked as DONE?”.  The computing script could also mark experiments as “LEGACY”, “INTERRUPTED”, or “ERROR” depending on the conditions at runtime.  If they have not yet been computed, they are marked in the database as “NOT STARTED”. So MasterNode asks for everything that’s not “DONE”, and forces a re-compute if a simulation errored or was cut short.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The central database, via the API, responds with a JSON object that contains the number of cores and memory needed for the next experiment (but not the other experimental parameters like number of training examples or spatial resolution).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MasterNode parses the JSON and then uses the &lt;code class=&quot;highlighter-rouge&quot;&gt;gcloud&lt;/code&gt; tools to create a pool of computers that have the memory and number of cores specified by the database response.  This pool of virtual instances is automatically created with a startup script that installs the necessary software and files to run the computing experiments.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;running-the-simulations-reporting-the-results&quot;&gt;Running the simulations; Reporting the results&lt;/h4&gt;
&lt;p&gt;Now that I have the pool of virtual instances at my disposal, I can use them to run the SDM simulations, time their execution, and report back to the central database.  There are typically between 160 and 400 simulations to be done for every computing configuration (cores &amp;amp; memory), so on each node is an inner loop that looks like this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Startup script installs git, mysql, and R.  Git clones the most recent version of the project repository which has all of the files needed for the computation.  R starts execution of the timeSDM.R script which controls the flow of execution for this node.&lt;/li&gt;
  &lt;li&gt;RScript queries the central database to ask “I am a compute of x cores and y GB memory, what experiments can I do?”.&lt;/li&gt;
  &lt;li&gt;The database responds with a single JSON row that contains all of the necessary parameters to actually run the SDM simulation (spatial resolution, taxon, number of training examples, etc).&lt;/li&gt;
  &lt;li&gt;RSCript parses the response and loads the correct set of variables, then runs the SDM model.
    &lt;ol&gt;
      &lt;li&gt;Fit the model (fitTime)&lt;/li&gt;
      &lt;li&gt;Project the model to AD 2100 (predictTime)&lt;/li&gt;
      &lt;li&gt;Evaluate accuracy (accuracyTime)&lt;/li&gt;
      &lt;li&gt;Return overall time, fitTime, predictTime, accuracyTime, and several measures of accuracy&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;RScript reports results back to the database.&lt;/li&gt;
  &lt;li&gt;Repeat until no experiments that are not “DONE” remain to be completed by a computer of this number cores and amount of memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If an instance gets shut down due to preemption (or my incompetence) a shutdown script will be fired. This script records in the database that the experiment was cut off (INTERRUPTED) at some point before successful completion, and that it should be completed again in the future.&lt;/p&gt;

&lt;h4 id=&quot;managing-virtual-infrastructure&quot;&gt;Managing virtual infrastructure&lt;/h4&gt;
&lt;p&gt;Because Google charges you by the minute as you use their servers, and because I have to do a lot of different experiments and don’t have that much time do them, it is ideal to automatically tear down the servers and start a new pool as soon as one computing configuration has finished. So, while the computing nodes are doing their computing thing, the MasterNode is doing this:
1.  Repeated polling every 30 seconds:
    1.  MasterNode used the API to ask the database “What percentage of the experiments in this group have been completed?”
    2.  The database responds with a percentage (“DONE” / total)
2.  If the percentage is 100%, everything has been completed, so MasterNode will use &lt;code class=&quot;highlighter-rouge&quot;&gt;gcloud&lt;/code&gt; to delete the individual server instances, the instance group pool the they are part of, and the template used to create the instances.  After this, only the Master Compute Node server with the database on it still remains in my pool of Google resources.
3.  Repeat.  Configure and build a new pool of instances for the next memory/cores combination.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This method so far works pretty well.  Sometimes, the shutdown scripts don’t actually fire (there are known bugs), and so I have one or two experiments that are continuously marked as “STARTED”.  This is a problem for the MasterNode, because the database will continuously report something link 99.75% complete, but will never reach 100%.  When this happens, I need to go in and manually mark the session as closed and the experiments as INTERRUPTED, and then the normal flow of execution can continue.  So I have to remain watchful, but I don’t have to do everything.&lt;/p&gt;
</description>
        <pubDate>Thu, 16 Jun 2016 08:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/16/Computing-Configuration-Update.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/16/Computing-Configuration-Update.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Managing Long-Running Processes on Linux</title>
        <description>&lt;p&gt;In my work, I have several times encountered the need to run a script for an extended period of time, or as a daemon (always running as a service).  Whether you’re on your own personal computer or SSHed into a virtual machine in the cloud, managing processes that take a long time can be annoying. If you finish your work day and close your laptop, you’re going to stop your script.  In the cloud (or I guess on a desktop/personal server too) you can take a couple steps to run scripts as services that will not stop when you end your work day.  There are a couple of ways of doing it that I’ve found.  Here are two that matched my needs.&lt;/p&gt;

&lt;h3 id=&quot;forever&quot;&gt;Forever&lt;/h3&gt;

&lt;h4 id=&quot;scenario&quot;&gt;Scenario:&lt;/h4&gt;
&lt;p&gt;I have a node.js based api server (to be discussed in a future post) that runs common sql queries for me so I can see the progress of my experiments over HTTP instead of typing out the commands manually.  I wrote the app, and can run it with &lt;code&gt;node app.js&lt;/code&gt;.  Works great – until I log off of SSH.  Because its a web server, I want it to be running all the time, so I can see it from anywhere on the internet, not just when I am actively working on my projects.  So I need a way to make this run automatically, 24/7.&lt;/p&gt;

&lt;h4 id=&quot;solution&quot;&gt;Solution:&lt;/h4&gt;
&lt;p&gt;Forever is a super simple tool designed to make your app run continuously. It is both written in node and designed to run node apps, so it’s a good choice for our scenario.  Also, it’s &lt;a href=&quot;https://github.com/foreverjs/forever&quot;&gt;open source&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To install &lt;code&gt;forever&lt;/code&gt;, I will use the node package manager (npm).  &lt;code&gt;cd&lt;/code&gt; into your application’s project directory, then install the package with &lt;code&gt;[sudo] npm install forever -g&lt;/code&gt;.  The g flag should make the package available globally, and, importantly for us, available as a command line tool.  Y&lt;/p&gt;

&lt;p&gt;So I’ve got my app, I’ve debugged it both on a localhost and on the remote machine I’ll be hosting the server on, so I know it is going to work, and I’ve installed the package.  To start the process &lt;code&gt;cd&lt;/code&gt; into the directory you’re going to use, and then start the app with &lt;code&gt;forever [APPLICATION-NAME].js&lt;/code&gt;. In my case, it’s &lt;code&gt;forever app.js&lt;/code&gt;.  And that’s it.  You can check by examining the logs, or, more simply, going to wherever your application is serving content.  I have mine running on the 8080 port of the server I’m using as the master node / database of my project.  If I go there in my web browser, I see that my node project is running as expected.&lt;/p&gt;

&lt;p&gt;If you update your app, you’ll need to restart the service.  You can list the running services controlled by &lt;code&gt;forever&lt;/code&gt; by using &lt;code&gt;forever list&lt;/code&gt;.  Find your process (if you have more than one running), and then stop by &lt;code&gt; forever stop [ID]&lt;/code&gt;. If you have exactly one process running, you can shortcut the listing and just run &lt;code&gt;forever stop 0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In all, &lt;code&gt;forever&lt;/code&gt; is a very simple and easy to use solution for running node apps as services on virtual machines.  I think that we could use it to run scripts in other languages, like python, but I decided to use another tool for that.&lt;/p&gt;

&lt;h3 id=&quot;supervisor&quot;&gt;Supervisor&lt;/h3&gt;

&lt;h4 id=&quot;scenario-1&quot;&gt;Scenario:&lt;/h4&gt;
&lt;p&gt;I have a python script that I call MasterNode which controls the creation and deletion of all of the other virtual machines that I need to work with for my project.  MasterNode starts, figures out what the next core/memory combination is that has not be finished yet, then fires up a group of servers to do this job. When all of the experiments are complete, the script kills them, deletes them, and starts a new configuration.  The whole point of having MasterNode is that I don’t have to worry about specifying which configuration comes next, I can just sit back and relax while MasterNode figures it out for me.  This only works, though, if MasterNode is running, which stops after I log out of the virtual machine. I like sitting back and relaxing – So clearly I need a way to make it run continuously. I found a tool called &lt;code&gt;supervisor&lt;/code&gt; which is very similar to &lt;code&gt;forever&lt;/code&gt; but looks more robust and versatile.&lt;/p&gt;

&lt;h4 id=&quot;solution-1&quot;&gt;Solution:&lt;/h4&gt;
&lt;p&gt;Getting it up and running was slightly more challenging – &lt;a href=&quot;https://serversforhackers.com/monitoring-processes-with-supervisord&quot;&gt;this post&lt;/a&gt; was very helpful. To install &lt;code&gt;supervisor&lt;/code&gt; use &lt;code&gt;apt-get&lt;/code&gt;.  Specifically, we will use &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y supervisor&lt;/code&gt;.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;-y&lt;/code&gt; flag indicates that you will answer yes to any question the system asks you (like ‘Are you sure you want to install this package?’).  Now we’ve installed the package, we need to define a program for it to run.&lt;/p&gt;

&lt;p&gt;Create a new program configuration file using &lt;code class=&quot;highlighter-rouge&quot;&gt;nano [APPLICATION-NAME].conf&lt;/code&gt;.  Inside of the file, you will write the details about where the program executable is located, whether it restarts, etc.  Details are confusing but can be found &lt;a href=&quot;http://supervisord.org/configuration.html&quot;&gt;here&lt;/a&gt;.  My configuration file looks like this: (comments are added, remove them)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;table style=&quot;border-spacing: 0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot; style=&quot;text-align: right&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;program:masterNode]  &lt;span class=&quot;c&quot;&gt;## name your process&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;python -u masterNode.py &lt;span class=&quot;c&quot;&gt;## this is the command that supervisor will use to start your script&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;directory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/rstudio/thesis-scripts/python  &lt;span class=&quot;c&quot;&gt;## this is the directory that your script lives in&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;stdout_logfile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/rstudio/thesis-scripts/logs/masterNode.log &lt;span class=&quot;c&quot;&gt;## this is where the output of your script will go&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;redirect_stderr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## put errors into your log too&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, move the configuration file into the directory used by the supervisor program:&lt;/p&gt;

&lt;pre&gt;
sudo mv [APPLICATION-NAME].conf /etc/supervisor/conf.d
&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;conf.d&lt;/code&gt; directory holds all of the programs you want to be run by supervisor.  When it starts, it will look in this directory for any &lt;code class=&quot;highlighter-rouge&quot;&gt;.conf&lt;/code&gt; files.&lt;/p&gt;

&lt;p&gt;The first time you run the program, it might try to automatically start your program.  That’s cool, but if it doesn’t you can start your script running under supervisor with these steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open the supervisor tool with &lt;code class=&quot;highlighter-rouge&quot;&gt;supervisorctl&lt;/code&gt;, which opens up a new supervisor shell.&lt;/li&gt;
  &lt;li&gt;Load any new configuration files with &lt;code class=&quot;highlighter-rouge&quot;&gt;reread&lt;/code&gt;, which will print out a list of available programs that you could run.&lt;/li&gt;
  &lt;li&gt;Start your program with &lt;code class=&quot;highlighter-rouge&quot;&gt;add [programName]&lt;/code&gt;, which will put your program under supervisor’s control.  Now you can sit back and relax :)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can review the logs of your scripts by looking at them directly in the folder where you put them, or better, you can view them in real time with &lt;code class=&quot;highlighter-rouge&quot;&gt;tail -f [programName]&lt;/code&gt; from the supervisor shell.&lt;/p&gt;

&lt;p&gt;To stop the program, you can &lt;code class=&quot;highlighter-rouge&quot;&gt;stop [programName]&lt;/code&gt;, which puts your application into paused mode. If you want to remove it from supervisor’s control all together, you can then enter &lt;code class=&quot;highlighter-rouge&quot;&gt;remove [programName]&lt;/code&gt; to take it off of supervisor’s list.&lt;/p&gt;

&lt;p&gt;This technique seemed to work well for me.  It auto-starts and auto-restarts your programs if they get cut off.  The package also comes with a web based manager on port 9001, which you can use to control your processes remotely.  Nifty.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;There are many ways to control your long running processes.  One of the ones I did not mention here is to create the script as its own service.  However, I like the management and flexibility of the tools I discussed here.  Both of these tools would work to do either job too, so I think its really a matter of preference to pick one out.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Jun 2016 11:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/12/managing-long-running-linux-processes.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/12/managing-long-running-linux-processes.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Startup and Shutdown Scripts in Google Cloud Compute Engine</title>
        <description>&lt;p&gt;In continuing my meditations on beginning to use the Google Cloud Computing platform, this post will describe the use of startup and shutdown scripts. If you want to start multiple instances that are all the same in terms of programs, data, etc (but perhaps of different size), you have two options. First, you could save your fully configured machine as an image, or more likely, as a snapshot.  Booting with this configuration is easy, just select the option from the menu when starting the new instance. Proceeding in this way has several potential drawbacks, however.  Most notably, it is very difficult to keep everything updated with this method.  Unless you manually update the snapshot pretty often, your software is going to be out of date.  Moreover, if you decide to make a small change in the scripts or programs you’re running on the instance, you will need to make an update to the snapshot.&lt;/p&gt;

&lt;p&gt;Instead of using a pre-configured snapshot, you can make use of Google Cloud’s ability to automatically run a script at boot time.  Using this script allows us to automatically install the latest version our programs using the &lt;code&gt;apt-get&lt;/code&gt; or other linux package manager, and if using version control software like git, download the latest working version of the scripts.  Google provides fairly easy-to-follow documentation about startup scripts &lt;a href=&quot;https://cloud.google.com/compute/docs/startupscript&quot;&gt;here&lt;/a&gt;.  In the same vein, Google provides a beta feature to run a script on machine termination, so you can save your work, etc.  There are a variety of limitations described in the &lt;a href=&quot;https://cloud.google.com/compute/docs/shutdownscript&quot;&gt;shutdown-script documentation&lt;/a&gt;, but they can be handy to run short cleanup jobs.&lt;/p&gt;

&lt;p&gt;As noted in the documentation, you can write the script in a number of languages by changing the shebang line at the top of the script.  To run a bash script, the first line of the script should be &lt;code&gt;#! /bin/bash&lt;/code&gt;.  A python script could be run by changing this line to &lt;code&gt;#!/usr/bin/python&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;adding-a-script-to-a-single-instance&quot;&gt;Adding a script to a single instance&lt;/h3&gt;
&lt;p&gt;If you want to add a startup script to a single instance that is currently running:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Click on the instance properties in the Console&lt;/li&gt;
  &lt;li&gt;Click on edit, at the top&lt;/li&gt;
  &lt;li&gt;Scroll to the ‘Custom Metadata’ section&lt;/li&gt;
  &lt;li&gt;For key, enter &lt;code&gt;startup-script&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;In the value field, write your script&lt;/li&gt;
  &lt;li&gt;Click save at the bottom and restart the instance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;adding-a-script-to-all-instances-in-your-project&quot;&gt;Adding a script to all instances in your project:&lt;/h3&gt;
&lt;p&gt;These items will be applied to all &lt;em&gt;new&lt;/em&gt; instances in your project.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open the Cloud Computing Console&lt;/li&gt;
  &lt;li&gt;Click ‘Metadata’ in the lefthand navigation bar&lt;/li&gt;
  &lt;li&gt;Click ‘Edit’ at the top&lt;/li&gt;
  &lt;li&gt;Click ‘Add item’ to add a new key value pair&lt;/li&gt;
  &lt;li&gt;As above, for key enter &lt;code&gt;startup-script&lt;/code&gt;, and for value write your script&lt;/li&gt;
  &lt;li&gt;Save and close.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It can be confusing sometimes because the metadata added here in this panel is automatically applied to all of your new instances.  You cannot see these key-value pairs in the metadata section of instance, so just remember it’s there.&lt;/p&gt;

&lt;h4 id=&quot;adding-a-shutdown-script&quot;&gt;Adding a shutdown script&lt;/h4&gt;
&lt;p&gt;Follow the same steps as to run a startup script, but instead of the &lt;code&gt;startup-script&lt;/code&gt; key, put &lt;code&gt;shutdown-script&lt;/code&gt;.  Super easy.  Be warned, though, that shutdown scripts are not guaranteed to run, Google completes them on a ‘best effort basis.’&lt;/p&gt;

&lt;h3 id=&quot;my-startup-script&quot;&gt;My Startup Script&lt;/h3&gt;
&lt;p&gt;I’ll describe my startup script and what I choose to do for each instance to set them up to run the experiments I am working with.  In general, I download, install, and update a bunch of software, I configure the google cloud proxy, and then I clone my github repository which contains all of my working files.  Then, when all is complete, I automatically start my experiments to run through an &lt;code&gt;Rscript&lt;/code&gt; and then terminate the machine when they’re complete.  The script is automatically run as root, from the root directory, so all of the &lt;code&gt;sudo&lt;/code&gt;’s are repetitive.&lt;/p&gt;

&lt;pre&gt;#! /bin/bash

--&amp;gt;Tell the computer I&#39;m working in a bash script

&lt;/pre&gt;

&lt;pre&gt;sudo apt-get update

--&amp;gt;Update the base packages that are installed on the instance
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get -y install git
--&amp;gt;Install Git version control.  
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-y&lt;/code&gt; flag automatically answers &lt;code&gt;y&lt;/code&gt; to any queries presented by the download manager (i.e., are you sure you want to download this package? y/n).&lt;/p&gt;

&lt;pre&gt;sudo apt-get -y install r-base
--&amp;gt;Install R
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get install -y gdebi-core
--&amp;gt;Install a utility dependency for use in installing RStudio
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get install -y aptitude
--&amp;gt; Install the aptitude package manager
&lt;/pre&gt;

&lt;p&gt;Aptitude is a &lt;a href=&quot;http://askubuntu.com/questions/1743/is-aptitude-still-considered-superior-to-apt-get&quot;&gt;different&lt;/a&gt; package manager for linux systems.  This is the only one I could find that has a gdal package.&lt;/p&gt;

&lt;pre&gt;sudo aptitude install -y libgdal-dev
sudo aptitude install -y libproj-dev

--&amp;gt;Install gdal and proj4 libraries.
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get install -y  libmariadb-client-lgpl-dev

--&amp;gt;Install a database dependency necessary for installing RMySQL
&lt;/pre&gt;

&lt;pre&gt;
wget https://download2.rstudio.org/rstudio-server-0.99.902-amd64.deb
sudo gdebi -y rstudio-server-0.99.902-amd64.deb

--&amp;gt; Download and install RStudio
&lt;/pre&gt;

&lt;pre&gt;
wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64
mv cloud_sql_proxy.linux.amd64 cloud_sql_proxy
chmod +x cloud_sql_proxy

--&amp;gt;Download the cloud proxy script into your root directory and make it an executable.
&lt;/pre&gt;

&lt;pre&gt;
sudo mkdir cloudsql; sudo chmod 777 cloudsql

--&amp;gt;Make a directory for your cloudsql proxy sockets
&lt;/pre&gt;

&lt;pre&gt;
rm -rf /home/rstudio/thesis-scripts

--&amp;gt;Remove any old version of the script directory I&#39;ll be using.
&lt;/pre&gt;

&lt;pre&gt;git clone http://github.com/scottsfarley93/thesis-scripts /home/rstudio/thesis-scripts

--&amp;gt;Clone the latest and greatest version of my repo into my instance.
&lt;/pre&gt;

&lt;pre&gt;sudo ./cloud_sql_proxy  -dir=/cloudsql -instances=thesis-1329:us-central1:sdm-database-3 &amp;amp;

--&amp;gt;Start the cloudsql proxy so we can make database connections.
&lt;/pre&gt;

&lt;pre&gt;Rscript /home/rstudio/thesis-scripts/R/time_sdm.R 50 TRUE

--&amp;gt;Start the experiment script to run 50 iterations and then shutdown the machine.
&lt;/pre&gt;
</description>
        <pubDate>Sun, 05 Jun 2016 11:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/05/Startup-and-Shutdown-Scripts-GCE.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/05/Startup-and-Shutdown-Scripts-GCE.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
  </channel>
</rss>
